{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransferLearning_dachshund.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQ7kTAAux8FWFnlyg3Gc2c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konkuk-gaegul/3rd-Team-Project/blob/main/Modeling/TransferLearning_dachshund.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 닥스훈트\n",
        "- 9개 이미지 전처리 중, set 9에서 가장 좋은 성능을 보임\n",
        "- batch_size = 9, epoch = 100인 경우 Loss : 0.1858, Acc : 95.56%"
      ],
      "metadata": {
        "id": "kuywuEiCaB38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39a3b0f7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import copy\n",
        "import os, shutil\n",
        "import matplotlib as plt\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 객체"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "697e6051",
        "outputId": "c43dea2f-cf25-473c-ec1d-b5990c1410ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "닥스훈트 set_9\n",
            "698\n",
            "299\n",
            "---------\n"
          ]
        }
      ],
      "source": [
        "for i in range(9, 10):\n",
        "    base_dir = '/home/tutor/안재형/dog_pic/닥스훈트'\n",
        "    base_folder = os.path.join(base_dir, f'set_{i}')\n",
        "    # # 훈련셋, 검증셋, 테스트셋을 미리 분할하기 위한 폴더 경로\n",
        "    train_path = os.path.join(base_folder, 'train')\n",
        "    os.mkdir( train_path)\n",
        "\n",
        "    test_path = os.path.join(base_folder, 'test')\n",
        "    os.mkdir( test_path)\n",
        "\n",
        "    # # train 정상 폴더\n",
        "    train_nor_path = os.path.join(train_path, 'nor')\n",
        "    os.mkdir( train_nor_path)\n",
        "\n",
        "    # # train 비만 폴더\n",
        "    train_fat_path = os.path.join(train_path, 'fat')\n",
        "    os.mkdir( train_fat_path)\n",
        "\n",
        "    # # test 정상 폴더\n",
        "    test_nor_path = os.path.join(test_path, 'nor')\n",
        "    os.mkdir( test_nor_path)\n",
        "\n",
        "    # # test 비만 폴더\n",
        "    test_fat_path = os.path.join(test_path, 'fat')\n",
        "    os.mkdir( test_fat_path)\n",
        "    \n",
        "    nor_path = os.path.join(base_folder, '전처리_정상')\n",
        "    fat_path = os.path.join(base_folder, '전처리_비만')\n",
        "\n",
        "    nor_list = os.listdir(nor_path)\n",
        "    fat_list = os.listdir(fat_path)\n",
        "    print(f'닥스훈트 set_{i}')\n",
        "    print(len(nor_list))\n",
        "    print(len(fat_list))\n",
        "    print('---------')\n",
        "    \n",
        "    # train : 75%, test : 25%\n",
        "    # 정상 이미지 복사\n",
        "    for i in range(len(nor_list)):\n",
        "        src_path = os.path.join(nor_path, nor_list[i])\n",
        "\n",
        "        if i < len(nor_list)*0.75:\n",
        "            dst_path = os.path.join(train_nor_path, f'nor_{i}.jpg')\n",
        "            shutil.copyfile( src_path, dst_path )\n",
        "        else :\n",
        "            dst_path = os.path.join(test_nor_path, f'nor_{i}.jpg')\n",
        "            shutil.copyfile( src_path, dst_path )\n",
        "\n",
        "    # 비만 이미지 복사\n",
        "    for i in range(len(fat_list)):\n",
        "        src_path = os.path.join(fat_path, fat_list[i])\n",
        "\n",
        "        if i < len(fat_list)*0.75:\n",
        "            dst_path = os.path.join(train_fat_path, f'fat_{i}.jpg')\n",
        "            shutil.copyfile( src_path, dst_path )\n",
        "        else :\n",
        "            dst_path = os.path.join(test_fat_path, f'fat_{i}.jpg')\n",
        "            shutil.copyfile( src_path, dst_path )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7417019"
      },
      "source": [
        "# 학습 1\n",
        "- set_9 이미지 사용\n",
        "- batch_size를 5 ~ 14까지 반복, 최적의 성능을 찾는다.\n",
        "- 이미지 증식 활용\n",
        "- epoch = 50, 100, 150 각 3번 반복"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e656c56"
      },
      "outputs": [],
      "source": [
        "# Loss_Accuracy에 각 배치 사이즈마다의 성능을 넣어둔다\n",
        "Loss_Accuracy = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0c48ddd",
        "outputId": "799ed89d-11d6-4dbb-cb81-b5b1927cba72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------현재 set_9 1번째 학습-------------\n",
            "#0 Loss: 0.5076 Acc: 77.1696% Time: 6.2050s\n",
            "#1 Loss: 0.3333 Acc: 86.1148% Time: 12.0777s\n",
            "#2 Loss: 0.1885 Acc: 91.4553% Time: 17.9526s\n",
            "#3 Loss: 0.1847 Acc: 92.3899% Time: 23.6676s\n",
            "#4 Loss: 0.1641 Acc: 93.1909% Time: 29.4080s\n",
            "#5 Loss: 0.0852 Acc: 97.0628% Time: 35.1261s\n",
            "#6 Loss: 0.1400 Acc: 95.8612% Time: 40.7812s\n",
            "#7 Loss: 0.0977 Acc: 96.3952% Time: 46.8197s\n",
            "#8 Loss: 0.0543 Acc: 98.3979% Time: 52.8643s\n",
            "#9 Loss: 0.0827 Acc: 97.1963% Time: 58.7087s\n",
            "#10 Loss: 0.0563 Acc: 98.2644% Time: 64.7057s\n",
            "#11 Loss: 0.0699 Acc: 97.1963% Time: 70.6908s\n",
            "#12 Loss: 0.0841 Acc: 97.5968% Time: 76.5371s\n",
            "#13 Loss: 0.0268 Acc: 99.1989% Time: 82.3722s\n",
            "#14 Loss: 0.0475 Acc: 98.1308% Time: 88.2384s\n",
            "#15 Loss: 0.0185 Acc: 99.5995% Time: 94.0205s\n",
            "#16 Loss: 0.0368 Acc: 99.0654% Time: 99.7465s\n",
            "#17 Loss: 0.0215 Acc: 99.1989% Time: 105.9130s\n",
            "#18 Loss: 0.0314 Acc: 98.7984% Time: 111.7504s\n",
            "#19 Loss: 0.0160 Acc: 99.7330% Time: 117.6201s\n",
            "#20 Loss: 0.0123 Acc: 99.7330% Time: 123.5577s\n",
            "#21 Loss: 0.0260 Acc: 98.7984% Time: 129.3988s\n",
            "#22 Loss: 0.0151 Acc: 99.4660% Time: 135.1999s\n",
            "#23 Loss: 0.0161 Acc: 99.4660% Time: 140.9235s\n",
            "#24 Loss: 0.0184 Acc: 99.1989% Time: 146.8581s\n",
            "#25 Loss: 0.0064 Acc: 99.8665% Time: 152.5758s\n",
            "#26 Loss: 0.0081 Acc: 99.7330% Time: 158.7305s\n",
            "#27 Loss: 0.0114 Acc: 99.5995% Time: 164.3782s\n",
            "#28 Loss: 0.0146 Acc: 99.3324% Time: 170.1352s\n",
            "#29 Loss: 0.0117 Acc: 99.7330% Time: 175.8436s\n",
            "#30 Loss: 0.0086 Acc: 99.7330% Time: 181.3115s\n",
            "#31 Loss: 0.0096 Acc: 99.5995% Time: 187.0106s\n",
            "#32 Loss: 0.0210 Acc: 99.1989% Time: 192.8880s\n",
            "#33 Loss: 0.0277 Acc: 99.1989% Time: 198.6112s\n",
            "#34 Loss: 0.0351 Acc: 98.6649% Time: 204.2574s\n",
            "#35 Loss: 0.0128 Acc: 99.8665% Time: 209.8685s\n",
            "#36 Loss: 0.0040 Acc: 100.0000% Time: 215.6795s\n",
            "#37 Loss: 0.0049 Acc: 100.0000% Time: 221.6819s\n",
            "#38 Loss: 0.0076 Acc: 99.8665% Time: 227.4679s\n",
            "#39 Loss: 0.0038 Acc: 100.0000% Time: 233.3130s\n",
            "#40 Loss: 0.0025 Acc: 100.0000% Time: 239.0693s\n",
            "#41 Loss: 0.0074 Acc: 99.7330% Time: 245.1688s\n",
            "#42 Loss: 0.0088 Acc: 99.5995% Time: 251.0630s\n",
            "#43 Loss: 0.0155 Acc: 99.3324% Time: 256.8819s\n",
            "#44 Loss: 0.0026 Acc: 100.0000% Time: 262.7098s\n",
            "#45 Loss: 0.0031 Acc: 100.0000% Time: 268.5506s\n",
            "#46 Loss: 0.0020 Acc: 100.0000% Time: 274.2689s\n",
            "#47 Loss: 0.0051 Acc: 99.8665% Time: 280.0319s\n",
            "#48 Loss: 0.0029 Acc: 100.0000% Time: 285.4736s\n",
            "#49 Loss: 0.0009 Acc: 100.0000% Time: 291.2697s\n",
            "#50 Loss: 0.0009 Acc: 100.0000% Time: 297.0008s\n",
            "#51 Loss: 0.0150 Acc: 99.4660% Time: 302.7665s\n",
            "#52 Loss: 0.0022 Acc: 100.0000% Time: 308.5841s\n",
            "#53 Loss: 0.0042 Acc: 100.0000% Time: 314.2504s\n",
            "#54 Loss: 0.0074 Acc: 99.8665% Time: 320.2975s\n",
            "#55 Loss: 0.0039 Acc: 99.8665% Time: 326.2207s\n",
            "#56 Loss: 0.0032 Acc: 99.8665% Time: 332.0642s\n",
            "#57 Loss: 0.0013 Acc: 100.0000% Time: 337.9147s\n",
            "#58 Loss: 0.0033 Acc: 99.8665% Time: 343.4428s\n",
            "#59 Loss: 0.0033 Acc: 99.8665% Time: 349.0739s\n",
            "#60 Loss: 0.0017 Acc: 100.0000% Time: 355.0675s\n",
            "#61 Loss: 0.0015 Acc: 100.0000% Time: 360.9780s\n",
            "#62 Loss: 0.0042 Acc: 99.7330% Time: 366.8000s\n",
            "#63 Loss: 0.0094 Acc: 99.8665% Time: 372.7535s\n",
            "#64 Loss: 0.0028 Acc: 100.0000% Time: 378.2565s\n",
            "#65 Loss: 0.0021 Acc: 100.0000% Time: 384.2963s\n",
            "#66 Loss: 0.0012 Acc: 100.0000% Time: 390.1706s\n",
            "#67 Loss: 0.0010 Acc: 100.0000% Time: 395.6856s\n",
            "#68 Loss: 0.0006 Acc: 100.0000% Time: 401.2749s\n",
            "#69 Loss: 0.0012 Acc: 100.0000% Time: 406.8157s\n",
            "#70 Loss: 0.0022 Acc: 100.0000% Time: 412.3919s\n",
            "#71 Loss: 0.0022 Acc: 100.0000% Time: 418.2261s\n",
            "#72 Loss: 0.0017 Acc: 100.0000% Time: 423.9634s\n",
            "#73 Loss: 0.0008 Acc: 100.0000% Time: 429.7197s\n",
            "#74 Loss: 0.0035 Acc: 99.7330% Time: 435.2511s\n",
            "#75 Loss: 0.0027 Acc: 100.0000% Time: 440.9279s\n",
            "#76 Loss: 0.0011 Acc: 100.0000% Time: 446.6717s\n",
            "#77 Loss: 0.0005 Acc: 100.0000% Time: 452.2325s\n",
            "#78 Loss: 0.0009 Acc: 100.0000% Time: 457.8785s\n",
            "#79 Loss: 0.0012 Acc: 100.0000% Time: 463.5126s\n",
            "#80 Loss: 0.0009 Acc: 100.0000% Time: 469.3870s\n",
            "#81 Loss: 0.0005 Acc: 100.0000% Time: 475.3076s\n",
            "#82 Loss: 0.0015 Acc: 100.0000% Time: 481.0416s\n",
            "#83 Loss: 0.0005 Acc: 100.0000% Time: 487.1462s\n",
            "#84 Loss: 0.0006 Acc: 100.0000% Time: 492.8935s\n",
            "#85 Loss: 0.0006 Acc: 100.0000% Time: 498.5940s\n",
            "#86 Loss: 0.0226 Acc: 99.5995% Time: 504.7112s\n",
            "#87 Loss: 0.0038 Acc: 99.8665% Time: 510.5966s\n",
            "#88 Loss: 0.0026 Acc: 99.8665% Time: 516.4518s\n",
            "#89 Loss: 0.0019 Acc: 100.0000% Time: 522.1744s\n",
            "#90 Loss: 0.0019 Acc: 100.0000% Time: 527.9983s\n",
            "#91 Loss: 0.0029 Acc: 99.8665% Time: 533.7928s\n",
            "#92 Loss: 0.0034 Acc: 99.8665% Time: 539.5162s\n",
            "#93 Loss: 0.0012 Acc: 100.0000% Time: 545.2682s\n",
            "#94 Loss: 0.0014 Acc: 100.0000% Time: 551.0639s\n",
            "#95 Loss: 0.0005 Acc: 100.0000% Time: 556.9579s\n",
            "#96 Loss: 0.0009 Acc: 100.0000% Time: 562.6780s\n",
            "#97 Loss: 0.0006 Acc: 100.0000% Time: 568.5870s\n",
            "#98 Loss: 0.0014 Acc: 100.0000% Time: 574.4087s\n",
            "#99 Loss: 0.0006 Acc: 100.0000% Time: 580.2689s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.1786 Acc: 93.9516% Time: 1.0681s\n",
            "현재 Best는 epoch : 100 / iteration : 1번째 / Loss : 0.17863983664767968 \n",
            "-------------현재 set_9 2번째 학습-------------\n",
            "#0 Loss: 0.4423 Acc: 79.1722% Time: 5.9830s\n",
            "#1 Loss: 0.2781 Acc: 88.1175% Time: 11.6459s\n",
            "#2 Loss: 0.2249 Acc: 89.7196% Time: 17.4519s\n",
            "#3 Loss: 0.1459 Acc: 94.2590% Time: 23.1647s\n",
            "#4 Loss: 0.1747 Acc: 92.9239% Time: 29.2900s\n",
            "#5 Loss: 0.1003 Acc: 96.1282% Time: 35.3924s\n",
            "#6 Loss: 0.1107 Acc: 95.9947% Time: 41.2333s\n",
            "#7 Loss: 0.0990 Acc: 95.5941% Time: 46.9433s\n",
            "#8 Loss: 0.0960 Acc: 96.1282% Time: 53.0452s\n",
            "#9 Loss: 0.0668 Acc: 97.4633% Time: 58.9527s\n",
            "#10 Loss: 0.0738 Acc: 97.4633% Time: 64.7927s\n",
            "#11 Loss: 0.0619 Acc: 97.8638% Time: 70.6967s\n",
            "#12 Loss: 0.0514 Acc: 98.2644% Time: 76.5788s\n",
            "#13 Loss: 0.0443 Acc: 97.9973% Time: 82.5888s\n",
            "#14 Loss: 0.0361 Acc: 98.6649% Time: 88.6721s\n",
            "#15 Loss: 0.0778 Acc: 97.3298% Time: 94.4343s\n",
            "#16 Loss: 0.0536 Acc: 98.3979% Time: 100.1670s\n",
            "#17 Loss: 0.0258 Acc: 99.1989% Time: 105.9360s\n",
            "#18 Loss: 0.0147 Acc: 99.3324% Time: 111.7965s\n",
            "#19 Loss: 0.0513 Acc: 98.3979% Time: 117.3916s\n",
            "#20 Loss: 0.0171 Acc: 99.5995% Time: 123.2198s\n",
            "#21 Loss: 0.0186 Acc: 99.3324% Time: 128.9955s\n",
            "#22 Loss: 0.0201 Acc: 99.4660% Time: 134.8534s\n",
            "#23 Loss: 0.0076 Acc: 99.8665% Time: 140.4739s\n",
            "#24 Loss: 0.0161 Acc: 99.1989% Time: 146.2142s\n",
            "#25 Loss: 0.0100 Acc: 100.0000% Time: 152.0761s\n",
            "#26 Loss: 0.0225 Acc: 99.1989% Time: 157.8835s\n",
            "#27 Loss: 0.0090 Acc: 99.7330% Time: 163.6668s\n",
            "#28 Loss: 0.0167 Acc: 99.3324% Time: 169.7328s\n",
            "#29 Loss: 0.0065 Acc: 99.8665% Time: 175.4858s\n",
            "#30 Loss: 0.0078 Acc: 99.8665% Time: 181.4433s\n",
            "#31 Loss: 0.0050 Acc: 100.0000% Time: 187.2045s\n",
            "#32 Loss: 0.0084 Acc: 99.7330% Time: 193.2983s\n",
            "#33 Loss: 0.0045 Acc: 100.0000% Time: 198.9233s\n",
            "#34 Loss: 0.0197 Acc: 99.7330% Time: 204.6466s\n",
            "#35 Loss: 0.0065 Acc: 100.0000% Time: 210.5171s\n",
            "#36 Loss: 0.0091 Acc: 99.5995% Time: 216.5298s\n",
            "#37 Loss: 0.0030 Acc: 100.0000% Time: 222.3186s\n",
            "#38 Loss: 0.0046 Acc: 99.8665% Time: 228.1199s\n",
            "#39 Loss: 0.0027 Acc: 100.0000% Time: 233.8361s\n",
            "#40 Loss: 0.0090 Acc: 99.7330% Time: 239.7091s\n",
            "#41 Loss: 0.0148 Acc: 99.4660% Time: 245.5560s\n",
            "#42 Loss: 0.0049 Acc: 99.8665% Time: 251.3654s\n",
            "#43 Loss: 0.0021 Acc: 100.0000% Time: 257.0528s\n",
            "#44 Loss: 0.0019 Acc: 100.0000% Time: 262.9869s\n",
            "#45 Loss: 0.0026 Acc: 99.8665% Time: 268.5459s\n",
            "#46 Loss: 0.0050 Acc: 99.8665% Time: 274.1140s\n",
            "#47 Loss: 0.0105 Acc: 99.5995% Time: 280.1098s\n",
            "#48 Loss: 0.0024 Acc: 100.0000% Time: 285.8650s\n",
            "#49 Loss: 0.0024 Acc: 100.0000% Time: 291.4364s\n",
            "#50 Loss: 0.0016 Acc: 100.0000% Time: 297.2403s\n",
            "#51 Loss: 0.0043 Acc: 99.8665% Time: 303.2399s\n",
            "#52 Loss: 0.0056 Acc: 99.5995% Time: 308.9818s\n",
            "#53 Loss: 0.0021 Acc: 100.0000% Time: 314.6879s\n",
            "#54 Loss: 0.0018 Acc: 100.0000% Time: 320.5923s\n",
            "#55 Loss: 0.0083 Acc: 99.7330% Time: 326.2213s\n",
            "#56 Loss: 0.0179 Acc: 99.4660% Time: 331.8928s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#57 Loss: 0.0077 Acc: 99.7330% Time: 337.6022s\n",
            "#58 Loss: 0.0045 Acc: 99.8665% Time: 343.5396s\n",
            "#59 Loss: 0.0067 Acc: 99.7330% Time: 349.5651s\n",
            "#60 Loss: 0.0199 Acc: 99.3324% Time: 355.3036s\n",
            "#61 Loss: 0.0043 Acc: 100.0000% Time: 361.1434s\n",
            "#62 Loss: 0.0019 Acc: 100.0000% Time: 366.9815s\n",
            "#63 Loss: 0.0020 Acc: 100.0000% Time: 372.7931s\n",
            "#64 Loss: 0.0015 Acc: 100.0000% Time: 378.8665s\n",
            "#65 Loss: 0.0021 Acc: 100.0000% Time: 384.6168s\n",
            "#66 Loss: 0.0012 Acc: 100.0000% Time: 390.2200s\n",
            "#67 Loss: 0.0009 Acc: 100.0000% Time: 396.0092s\n",
            "#68 Loss: 0.0009 Acc: 100.0000% Time: 401.9595s\n",
            "#69 Loss: 0.0070 Acc: 99.7330% Time: 407.9967s\n",
            "#70 Loss: 0.0040 Acc: 99.8665% Time: 413.7629s\n",
            "#71 Loss: 0.0012 Acc: 100.0000% Time: 419.7039s\n",
            "#72 Loss: 0.0106 Acc: 99.4660% Time: 425.4848s\n",
            "#73 Loss: 0.0036 Acc: 100.0000% Time: 431.2541s\n",
            "#74 Loss: 0.0174 Acc: 99.5995% Time: 437.2526s\n",
            "#75 Loss: 0.0039 Acc: 99.8665% Time: 443.0523s\n",
            "#76 Loss: 0.0019 Acc: 100.0000% Time: 448.7905s\n",
            "#77 Loss: 0.0076 Acc: 99.7330% Time: 454.8370s\n",
            "#78 Loss: 0.0116 Acc: 99.4660% Time: 460.5398s\n",
            "#79 Loss: 0.0050 Acc: 99.8665% Time: 466.3617s\n",
            "#80 Loss: 0.0019 Acc: 100.0000% Time: 472.4144s\n",
            "#81 Loss: 0.0016 Acc: 100.0000% Time: 478.2140s\n",
            "#82 Loss: 0.0027 Acc: 100.0000% Time: 484.2259s\n",
            "#83 Loss: 0.0006 Acc: 100.0000% Time: 490.3403s\n",
            "#84 Loss: 0.0024 Acc: 100.0000% Time: 496.0555s\n",
            "#85 Loss: 0.0008 Acc: 100.0000% Time: 501.6950s\n",
            "#86 Loss: 0.0091 Acc: 99.5995% Time: 507.3137s\n",
            "#87 Loss: 0.0041 Acc: 99.7330% Time: 512.9733s\n",
            "#88 Loss: 0.0031 Acc: 99.8665% Time: 518.7155s\n",
            "#89 Loss: 0.0023 Acc: 100.0000% Time: 524.4882s\n",
            "#90 Loss: 0.0020 Acc: 100.0000% Time: 530.4199s\n",
            "#91 Loss: 0.0040 Acc: 99.7330% Time: 536.2280s\n",
            "#92 Loss: 0.0011 Acc: 100.0000% Time: 542.1025s\n",
            "#93 Loss: 0.0011 Acc: 100.0000% Time: 548.0560s\n",
            "#94 Loss: 0.0028 Acc: 99.8665% Time: 553.7736s\n",
            "#95 Loss: 0.0014 Acc: 100.0000% Time: 559.4075s\n",
            "#96 Loss: 0.0007 Acc: 100.0000% Time: 565.0966s\n",
            "#97 Loss: 0.0009 Acc: 100.0000% Time: 570.9039s\n",
            "#98 Loss: 0.0080 Acc: 99.8665% Time: 576.8231s\n",
            "#99 Loss: 0.0024 Acc: 100.0000% Time: 582.5769s\n",
            "#100 Loss: 0.0016 Acc: 99.8665% Time: 588.3070s\n",
            "#101 Loss: 0.0005 Acc: 100.0000% Time: 594.1053s\n",
            "#102 Loss: 0.0009 Acc: 100.0000% Time: 599.7591s\n",
            "#103 Loss: 0.0027 Acc: 99.8665% Time: 605.5343s\n",
            "#104 Loss: 0.0014 Acc: 100.0000% Time: 611.3503s\n",
            "#105 Loss: 0.0009 Acc: 100.0000% Time: 617.6305s\n",
            "#106 Loss: 0.0004 Acc: 100.0000% Time: 623.7308s\n",
            "#107 Loss: 0.0007 Acc: 100.0000% Time: 629.5110s\n",
            "#108 Loss: 0.0022 Acc: 99.8665% Time: 635.5895s\n",
            "#109 Loss: 0.0016 Acc: 99.8665% Time: 641.4147s\n",
            "#110 Loss: 0.0101 Acc: 99.5995% Time: 647.0588s\n",
            "#111 Loss: 0.0011 Acc: 100.0000% Time: 653.0995s\n",
            "#112 Loss: 0.0036 Acc: 99.8665% Time: 658.9507s\n",
            "#113 Loss: 0.0008 Acc: 100.0000% Time: 664.8927s\n",
            "#114 Loss: 0.0007 Acc: 100.0000% Time: 670.9264s\n",
            "#115 Loss: 0.0009 Acc: 100.0000% Time: 676.6121s\n",
            "#116 Loss: 0.0005 Acc: 100.0000% Time: 682.3415s\n",
            "#117 Loss: 0.0005 Acc: 100.0000% Time: 688.1101s\n",
            "#118 Loss: 0.0022 Acc: 99.8665% Time: 694.0983s\n",
            "#119 Loss: 0.0007 Acc: 100.0000% Time: 700.0657s\n",
            "#120 Loss: 0.0009 Acc: 100.0000% Time: 705.9128s\n",
            "#121 Loss: 0.0003 Acc: 100.0000% Time: 711.8354s\n",
            "#122 Loss: 0.0012 Acc: 100.0000% Time: 717.6916s\n",
            "#123 Loss: 0.0015 Acc: 100.0000% Time: 723.4778s\n",
            "#124 Loss: 0.0011 Acc: 100.0000% Time: 729.2402s\n",
            "#125 Loss: 0.0006 Acc: 100.0000% Time: 734.9876s\n",
            "#126 Loss: 0.0003 Acc: 100.0000% Time: 740.7805s\n",
            "#127 Loss: 0.0004 Acc: 100.0000% Time: 746.6585s\n",
            "#128 Loss: 0.0004 Acc: 100.0000% Time: 752.6064s\n",
            "#129 Loss: 0.0023 Acc: 99.8665% Time: 758.2592s\n",
            "#130 Loss: 0.0016 Acc: 100.0000% Time: 764.2937s\n",
            "#131 Loss: 0.0004 Acc: 100.0000% Time: 770.3337s\n",
            "#132 Loss: 0.0003 Acc: 100.0000% Time: 776.4830s\n",
            "#133 Loss: 0.0002 Acc: 100.0000% Time: 782.5740s\n",
            "#134 Loss: 0.0009 Acc: 100.0000% Time: 788.2230s\n",
            "#135 Loss: 0.0009 Acc: 100.0000% Time: 794.1206s\n",
            "#136 Loss: 0.0098 Acc: 99.8665% Time: 800.1126s\n",
            "#137 Loss: 0.0009 Acc: 100.0000% Time: 806.2413s\n",
            "#138 Loss: 0.0012 Acc: 100.0000% Time: 811.9503s\n",
            "#139 Loss: 0.0004 Acc: 100.0000% Time: 817.9945s\n",
            "#140 Loss: 0.0004 Acc: 100.0000% Time: 823.9446s\n",
            "#141 Loss: 0.0019 Acc: 100.0000% Time: 829.7921s\n",
            "#142 Loss: 0.0005 Acc: 100.0000% Time: 835.7837s\n",
            "#143 Loss: 0.0006 Acc: 100.0000% Time: 841.8123s\n",
            "#144 Loss: 0.0005 Acc: 100.0000% Time: 847.7658s\n",
            "#145 Loss: 0.0005 Acc: 100.0000% Time: 853.8856s\n",
            "#146 Loss: 0.0345 Acc: 99.5995% Time: 859.9159s\n",
            "#147 Loss: 0.0061 Acc: 99.8665% Time: 865.7930s\n",
            "#148 Loss: 0.0023 Acc: 100.0000% Time: 871.9612s\n",
            "#149 Loss: 0.0009 Acc: 100.0000% Time: 877.9628s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2934 Acc: 91.1290% Time: 1.0853s\n",
            "현재 Best는 epoch : 150 / iteration : 2번째 / Loss : 0.2933852207833326 \n",
            "-------------현재 set_9 3번째 학습-------------\n",
            "#0 Loss: 0.4430 Acc: 78.5047% Time: 6.0236s\n",
            "#1 Loss: 0.2890 Acc: 87.5835% Time: 12.1703s\n",
            "#2 Loss: 0.2213 Acc: 90.1202% Time: 17.8579s\n",
            "#3 Loss: 0.2850 Acc: 88.3845% Time: 24.1902s\n",
            "#4 Loss: 0.1619 Acc: 93.4579% Time: 29.9716s\n",
            "#5 Loss: 0.1089 Acc: 95.5941% Time: 35.9722s\n",
            "#6 Loss: 0.1275 Acc: 95.5941% Time: 42.3085s\n",
            "#7 Loss: 0.0766 Acc: 97.0628% Time: 48.3783s\n",
            "#8 Loss: 0.0849 Acc: 97.0628% Time: 54.4088s\n",
            "#9 Loss: 0.0630 Acc: 97.7303% Time: 60.3417s\n",
            "#10 Loss: 0.0647 Acc: 97.4633% Time: 66.1628s\n",
            "#11 Loss: 0.0519 Acc: 98.1308% Time: 71.9062s\n",
            "#12 Loss: 0.0394 Acc: 98.3979% Time: 77.6811s\n",
            "#13 Loss: 0.0560 Acc: 98.2644% Time: 83.7357s\n",
            "#14 Loss: 0.0327 Acc: 98.7984% Time: 89.6793s\n",
            "#15 Loss: 0.0328 Acc: 98.5314% Time: 95.9394s\n",
            "#16 Loss: 0.0237 Acc: 99.1989% Time: 101.9808s\n",
            "#17 Loss: 0.0184 Acc: 99.4660% Time: 107.7933s\n",
            "#18 Loss: 0.0313 Acc: 98.6649% Time: 113.8398s\n",
            "#19 Loss: 0.0234 Acc: 99.3324% Time: 119.8289s\n",
            "#20 Loss: 0.0178 Acc: 99.3324% Time: 125.9038s\n",
            "#21 Loss: 0.0245 Acc: 99.1989% Time: 132.0371s\n",
            "#22 Loss: 0.0104 Acc: 99.7330% Time: 137.9389s\n",
            "#23 Loss: 0.0264 Acc: 98.6649% Time: 143.7047s\n",
            "#24 Loss: 0.0243 Acc: 98.9319% Time: 150.1079s\n",
            "#25 Loss: 0.0151 Acc: 99.5995% Time: 156.1294s\n",
            "#26 Loss: 0.0119 Acc: 99.5995% Time: 161.8823s\n",
            "#27 Loss: 0.0117 Acc: 99.4660% Time: 167.6931s\n",
            "#28 Loss: 0.0075 Acc: 99.8665% Time: 173.5866s\n",
            "#29 Loss: 0.0096 Acc: 99.8665% Time: 179.6696s\n",
            "#30 Loss: 0.0115 Acc: 99.4660% Time: 185.5680s\n",
            "#31 Loss: 0.0220 Acc: 99.3324% Time: 191.5275s\n",
            "#32 Loss: 0.0125 Acc: 99.8665% Time: 197.5496s\n",
            "#33 Loss: 0.0273 Acc: 99.1989% Time: 203.3146s\n",
            "#34 Loss: 0.0341 Acc: 98.7984% Time: 209.2184s\n",
            "#35 Loss: 0.0132 Acc: 99.4660% Time: 215.0490s\n",
            "#36 Loss: 0.0058 Acc: 100.0000% Time: 221.0489s\n",
            "#37 Loss: 0.0059 Acc: 99.8665% Time: 226.7766s\n",
            "#38 Loss: 0.0141 Acc: 99.4660% Time: 232.9309s\n",
            "#39 Loss: 0.0094 Acc: 99.7330% Time: 238.7950s\n",
            "#40 Loss: 0.0058 Acc: 99.7330% Time: 244.6857s\n",
            "#41 Loss: 0.0074 Acc: 99.7330% Time: 250.4925s\n",
            "#42 Loss: 0.0028 Acc: 100.0000% Time: 256.4915s\n",
            "#43 Loss: 0.0043 Acc: 99.8665% Time: 262.4921s\n",
            "#44 Loss: 0.0032 Acc: 99.8665% Time: 268.3523s\n",
            "#45 Loss: 0.0017 Acc: 100.0000% Time: 274.3325s\n",
            "#46 Loss: 0.0026 Acc: 99.8665% Time: 280.2140s\n",
            "#47 Loss: 0.0074 Acc: 99.5995% Time: 285.8447s\n",
            "#48 Loss: 0.0084 Acc: 99.5995% Time: 291.7897s\n",
            "#49 Loss: 0.0023 Acc: 100.0000% Time: 297.5095s\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2076 Acc: 94.3548% Time: 1.0127s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "현재 Best는 epoch : 50 / iteration : 3번째 / Loss : 0.20758715865574026 \n",
            "-------------현재 set_9 4번째 학습-------------\n",
            "#0 Loss: 0.4631 Acc: 75.7009% Time: 5.9129s\n",
            "#1 Loss: 0.3087 Acc: 87.5835% Time: 12.0145s\n",
            "#2 Loss: 0.1853 Acc: 93.1909% Time: 17.7806s\n",
            "#3 Loss: 0.2013 Acc: 92.1228% Time: 23.5736s\n",
            "#4 Loss: 0.1379 Acc: 95.3271% Time: 29.2426s\n",
            "#5 Loss: 0.1427 Acc: 94.7931% Time: 35.2852s\n",
            "#6 Loss: 0.0828 Acc: 96.9292% Time: 41.3489s\n",
            "#7 Loss: 0.1345 Acc: 93.8585% Time: 47.3088s\n",
            "#8 Loss: 0.0807 Acc: 96.5287% Time: 53.1952s\n",
            "#9 Loss: 0.0473 Acc: 98.6649% Time: 58.9849s\n",
            "#10 Loss: 0.0605 Acc: 97.8638% Time: 64.7450s\n",
            "#11 Loss: 0.0776 Acc: 97.0628% Time: 70.6223s\n",
            "#12 Loss: 0.0565 Acc: 98.1308% Time: 76.6015s\n",
            "#13 Loss: 0.0362 Acc: 98.6649% Time: 82.5840s\n",
            "#14 Loss: 0.0531 Acc: 98.6649% Time: 88.2523s\n",
            "#15 Loss: 0.0345 Acc: 98.7984% Time: 94.3671s\n",
            "#16 Loss: 0.0273 Acc: 99.0654% Time: 100.1369s\n",
            "#17 Loss: 0.0240 Acc: 99.1989% Time: 106.0454s\n",
            "#18 Loss: 0.0324 Acc: 98.7984% Time: 111.9114s\n",
            "#19 Loss: 0.0178 Acc: 99.4660% Time: 117.9265s\n",
            "#20 Loss: 0.0251 Acc: 98.9319% Time: 123.8893s\n",
            "#21 Loss: 0.0264 Acc: 98.7984% Time: 129.8360s\n",
            "#22 Loss: 0.0317 Acc: 98.9319% Time: 135.7675s\n",
            "#23 Loss: 0.0072 Acc: 100.0000% Time: 141.7293s\n",
            "#24 Loss: 0.0663 Acc: 97.9973% Time: 147.4945s\n",
            "#25 Loss: 0.0276 Acc: 99.0654% Time: 153.1955s\n",
            "#26 Loss: 0.0273 Acc: 98.9319% Time: 159.2300s\n",
            "#27 Loss: 0.0211 Acc: 99.1989% Time: 164.9454s\n",
            "#28 Loss: 0.0047 Acc: 100.0000% Time: 170.8684s\n",
            "#29 Loss: 0.0267 Acc: 98.9319% Time: 176.4926s\n",
            "#30 Loss: 0.0093 Acc: 99.8665% Time: 182.3148s\n",
            "#31 Loss: 0.0044 Acc: 100.0000% Time: 188.3995s\n",
            "#32 Loss: 0.0054 Acc: 99.8665% Time: 194.2961s\n",
            "#33 Loss: 0.0102 Acc: 99.5995% Time: 200.1887s\n",
            "#34 Loss: 0.0828 Acc: 97.8638% Time: 206.2601s\n",
            "#35 Loss: 0.0259 Acc: 99.1989% Time: 212.1785s\n",
            "#36 Loss: 0.0093 Acc: 99.7330% Time: 218.0767s\n",
            "#37 Loss: 0.0064 Acc: 99.7330% Time: 224.0835s\n",
            "#38 Loss: 0.0076 Acc: 99.8665% Time: 229.8094s\n",
            "#39 Loss: 0.0054 Acc: 100.0000% Time: 235.5522s\n",
            "#40 Loss: 0.0105 Acc: 99.5995% Time: 241.7256s\n",
            "#41 Loss: 0.0081 Acc: 99.7330% Time: 247.6439s\n",
            "#42 Loss: 0.0070 Acc: 99.8665% Time: 253.5625s\n",
            "#43 Loss: 0.0055 Acc: 99.8665% Time: 259.4522s\n",
            "#44 Loss: 0.0061 Acc: 99.8665% Time: 265.3263s\n",
            "#45 Loss: 0.0085 Acc: 99.7330% Time: 271.2265s\n",
            "#46 Loss: 0.0038 Acc: 100.0000% Time: 276.9366s\n",
            "#47 Loss: 0.0037 Acc: 100.0000% Time: 282.9334s\n",
            "#48 Loss: 0.0089 Acc: 99.7330% Time: 288.7526s\n",
            "#49 Loss: 0.0130 Acc: 99.7330% Time: 295.0906s\n",
            "#50 Loss: 0.0055 Acc: 99.8665% Time: 301.1433s\n",
            "#51 Loss: 0.0063 Acc: 99.7330% Time: 306.8998s\n",
            "#52 Loss: 0.0035 Acc: 99.8665% Time: 312.8289s\n",
            "#53 Loss: 0.0059 Acc: 99.8665% Time: 318.5624s\n",
            "#54 Loss: 0.0035 Acc: 99.8665% Time: 324.5917s\n",
            "#55 Loss: 0.0018 Acc: 100.0000% Time: 330.4811s\n",
            "#56 Loss: 0.0026 Acc: 100.0000% Time: 336.3249s\n",
            "#57 Loss: 0.0058 Acc: 99.8665% Time: 342.0606s\n",
            "#58 Loss: 0.0020 Acc: 100.0000% Time: 347.9100s\n",
            "#59 Loss: 0.0043 Acc: 99.8665% Time: 354.0688s\n",
            "#60 Loss: 0.0037 Acc: 99.8665% Time: 359.8146s\n",
            "#61 Loss: 0.0022 Acc: 100.0000% Time: 365.5603s\n",
            "#62 Loss: 0.0022 Acc: 100.0000% Time: 371.6249s\n",
            "#63 Loss: 0.0015 Acc: 100.0000% Time: 377.3652s\n",
            "#64 Loss: 0.0008 Acc: 100.0000% Time: 383.1076s\n",
            "#65 Loss: 0.0012 Acc: 100.0000% Time: 388.9828s\n",
            "#66 Loss: 0.0009 Acc: 100.0000% Time: 394.6417s\n",
            "#67 Loss: 0.0006 Acc: 100.0000% Time: 400.7355s\n",
            "#68 Loss: 0.0161 Acc: 99.3324% Time: 406.7955s\n",
            "#69 Loss: 0.0061 Acc: 99.7330% Time: 412.9845s\n",
            "#70 Loss: 0.0011 Acc: 100.0000% Time: 418.6987s\n",
            "#71 Loss: 0.0040 Acc: 99.8665% Time: 424.3725s\n",
            "#72 Loss: 0.0016 Acc: 100.0000% Time: 429.9757s\n",
            "#73 Loss: 0.0024 Acc: 100.0000% Time: 435.7289s\n",
            "#74 Loss: 0.0005 Acc: 100.0000% Time: 441.4335s\n",
            "#75 Loss: 0.0015 Acc: 100.0000% Time: 447.4431s\n",
            "#76 Loss: 0.0008 Acc: 100.0000% Time: 453.0875s\n",
            "#77 Loss: 0.0010 Acc: 100.0000% Time: 458.7691s\n",
            "#78 Loss: 0.0008 Acc: 100.0000% Time: 464.4423s\n",
            "#79 Loss: 0.0031 Acc: 99.8665% Time: 470.0982s\n",
            "#80 Loss: 0.0404 Acc: 98.7984% Time: 475.6495s\n",
            "#81 Loss: 0.0088 Acc: 99.5995% Time: 481.4473s\n",
            "#82 Loss: 0.0089 Acc: 99.4660% Time: 486.9912s\n",
            "#83 Loss: 0.0046 Acc: 100.0000% Time: 492.7782s\n",
            "#84 Loss: 0.0039 Acc: 100.0000% Time: 498.7000s\n",
            "#85 Loss: 0.0060 Acc: 99.8665% Time: 504.5420s\n",
            "#86 Loss: 0.0017 Acc: 100.0000% Time: 510.1114s\n",
            "#87 Loss: 0.0026 Acc: 100.0000% Time: 515.9509s\n",
            "#88 Loss: 0.0009 Acc: 100.0000% Time: 521.8373s\n",
            "#89 Loss: 0.0005 Acc: 100.0000% Time: 527.7875s\n",
            "#90 Loss: 0.0016 Acc: 100.0000% Time: 533.5050s\n",
            "#91 Loss: 0.0019 Acc: 99.8665% Time: 539.3020s\n",
            "#92 Loss: 0.0010 Acc: 100.0000% Time: 545.2919s\n",
            "#93 Loss: 0.0004 Acc: 100.0000% Time: 551.0168s\n",
            "#94 Loss: 0.0021 Acc: 100.0000% Time: 556.7447s\n",
            "#95 Loss: 0.0020 Acc: 100.0000% Time: 562.2436s\n",
            "#96 Loss: 0.0006 Acc: 100.0000% Time: 567.8340s\n",
            "#97 Loss: 0.0018 Acc: 100.0000% Time: 573.7639s\n",
            "#98 Loss: 0.0010 Acc: 100.0000% Time: 579.5488s\n",
            "#99 Loss: 0.0004 Acc: 100.0000% Time: 585.5372s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2067 Acc: 93.9516% Time: 1.0347s\n",
            "현재 Best는 epoch : 100 / iteration : 4번째 / Loss : 0.20669178503287125 \n",
            "-------------현재 set_9 5번째 학습-------------\n",
            "#0 Loss: 0.5008 Acc: 76.2350% Time: 5.5909s\n",
            "#1 Loss: 0.2906 Acc: 87.3164% Time: 11.3986s\n",
            "#2 Loss: 0.2217 Acc: 90.6542% Time: 17.3320s\n",
            "#3 Loss: 0.2029 Acc: 91.4553% Time: 23.3633s\n",
            "#4 Loss: 0.1229 Acc: 95.0601% Time: 29.2106s\n",
            "#5 Loss: 0.0949 Acc: 96.1282% Time: 35.1647s\n",
            "#6 Loss: 0.1181 Acc: 95.4606% Time: 40.9866s\n",
            "#7 Loss: 0.0992 Acc: 96.3952% Time: 46.6736s\n",
            "#8 Loss: 0.0430 Acc: 98.6649% Time: 52.7341s\n",
            "#9 Loss: 0.0738 Acc: 97.9973% Time: 58.3033s\n",
            "#10 Loss: 0.0714 Acc: 96.7957% Time: 64.6173s\n",
            "#11 Loss: 0.0285 Acc: 99.0654% Time: 70.3389s\n",
            "#12 Loss: 0.0397 Acc: 98.2644% Time: 76.1149s\n",
            "#13 Loss: 0.0408 Acc: 98.7984% Time: 81.9346s\n",
            "#14 Loss: 0.0558 Acc: 98.1308% Time: 87.5512s\n",
            "#15 Loss: 0.0485 Acc: 98.3979% Time: 93.5236s\n",
            "#16 Loss: 0.0462 Acc: 97.9973% Time: 99.4106s\n",
            "#17 Loss: 0.0346 Acc: 99.0654% Time: 105.2271s\n",
            "#18 Loss: 0.0422 Acc: 98.5314% Time: 110.9589s\n",
            "#19 Loss: 0.0583 Acc: 97.8638% Time: 116.8137s\n",
            "#20 Loss: 0.0353 Acc: 98.9319% Time: 122.8471s\n",
            "#21 Loss: 0.0216 Acc: 99.5995% Time: 128.6221s\n",
            "#22 Loss: 0.0179 Acc: 99.7330% Time: 134.5038s\n",
            "#23 Loss: 0.0154 Acc: 99.4660% Time: 140.2499s\n",
            "#24 Loss: 0.0247 Acc: 99.1989% Time: 146.0357s\n",
            "#25 Loss: 0.0281 Acc: 99.0654% Time: 152.0319s\n",
            "#26 Loss: 0.0117 Acc: 99.4660% Time: 158.0956s\n",
            "#27 Loss: 0.0148 Acc: 99.7330% Time: 164.0299s\n",
            "#28 Loss: 0.0255 Acc: 99.0654% Time: 169.7062s\n",
            "#29 Loss: 0.0064 Acc: 99.8665% Time: 175.6033s\n",
            "#30 Loss: 0.0054 Acc: 99.7330% Time: 181.5812s\n",
            "#31 Loss: 0.0082 Acc: 99.8665% Time: 187.2976s\n",
            "#32 Loss: 0.0063 Acc: 99.7330% Time: 192.8344s\n",
            "#33 Loss: 0.0074 Acc: 99.7330% Time: 198.4404s\n",
            "#34 Loss: 0.0099 Acc: 99.7330% Time: 204.2983s\n",
            "#35 Loss: 0.0333 Acc: 98.7984% Time: 210.4900s\n",
            "#36 Loss: 0.0193 Acc: 99.4660% Time: 216.5642s\n",
            "#37 Loss: 0.0125 Acc: 99.7330% Time: 222.6341s\n",
            "#38 Loss: 0.0038 Acc: 100.0000% Time: 228.5507s\n",
            "#39 Loss: 0.0041 Acc: 100.0000% Time: 234.5960s\n",
            "#40 Loss: 0.0055 Acc: 99.8665% Time: 240.3668s\n",
            "#41 Loss: 0.0273 Acc: 99.0654% Time: 246.4830s\n",
            "#42 Loss: 0.0145 Acc: 99.1989% Time: 252.4994s\n",
            "#43 Loss: 0.0066 Acc: 99.8665% Time: 258.1975s\n",
            "#44 Loss: 0.0039 Acc: 100.0000% Time: 263.7704s\n",
            "#45 Loss: 0.0039 Acc: 100.0000% Time: 269.6584s\n",
            "#46 Loss: 0.0033 Acc: 100.0000% Time: 275.6490s\n",
            "#47 Loss: 0.0044 Acc: 99.8665% Time: 281.8930s\n",
            "#48 Loss: 0.0045 Acc: 99.8665% Time: 287.8240s\n",
            "#49 Loss: 0.0028 Acc: 99.8665% Time: 293.8342s\n",
            "#50 Loss: 0.0015 Acc: 100.0000% Time: 299.6636s\n",
            "#51 Loss: 0.0017 Acc: 100.0000% Time: 305.5216s\n",
            "#52 Loss: 0.0008 Acc: 100.0000% Time: 311.5414s\n",
            "#53 Loss: 0.0016 Acc: 100.0000% Time: 317.2730s\n",
            "#54 Loss: 0.0005 Acc: 100.0000% Time: 323.1401s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#55 Loss: 0.0008 Acc: 100.0000% Time: 328.9190s\n",
            "#56 Loss: 0.0010 Acc: 100.0000% Time: 334.8029s\n",
            "#57 Loss: 0.0009 Acc: 100.0000% Time: 340.7293s\n",
            "#58 Loss: 0.0014 Acc: 100.0000% Time: 346.6976s\n",
            "#59 Loss: 0.0006 Acc: 100.0000% Time: 352.4358s\n",
            "#60 Loss: 0.0010 Acc: 100.0000% Time: 358.4587s\n",
            "#61 Loss: 0.0012 Acc: 100.0000% Time: 364.5831s\n",
            "#62 Loss: 0.0037 Acc: 99.8665% Time: 370.6761s\n",
            "#63 Loss: 0.0129 Acc: 99.4660% Time: 376.7686s\n",
            "#64 Loss: 0.0016 Acc: 100.0000% Time: 382.7151s\n",
            "#65 Loss: 0.0011 Acc: 100.0000% Time: 388.7287s\n",
            "#66 Loss: 0.0020 Acc: 99.8665% Time: 394.6817s\n",
            "#67 Loss: 0.0031 Acc: 100.0000% Time: 400.4813s\n",
            "#68 Loss: 0.0008 Acc: 100.0000% Time: 406.3282s\n",
            "#69 Loss: 0.0020 Acc: 100.0000% Time: 412.0202s\n",
            "#70 Loss: 0.0114 Acc: 99.7330% Time: 417.8405s\n",
            "#71 Loss: 0.0020 Acc: 100.0000% Time: 423.6126s\n",
            "#72 Loss: 0.0011 Acc: 100.0000% Time: 429.3922s\n",
            "#73 Loss: 0.0005 Acc: 100.0000% Time: 435.4234s\n",
            "#74 Loss: 0.0011 Acc: 100.0000% Time: 441.0146s\n",
            "#75 Loss: 0.0014 Acc: 100.0000% Time: 446.6473s\n",
            "#76 Loss: 0.0017 Acc: 100.0000% Time: 452.6225s\n",
            "#77 Loss: 0.0006 Acc: 100.0000% Time: 458.4628s\n",
            "#78 Loss: 0.0005 Acc: 100.0000% Time: 464.1500s\n",
            "#79 Loss: 0.0006 Acc: 100.0000% Time: 470.1537s\n",
            "#80 Loss: 0.0022 Acc: 100.0000% Time: 475.8333s\n",
            "#81 Loss: 0.0338 Acc: 98.7984% Time: 481.8192s\n",
            "#82 Loss: 0.0074 Acc: 99.7330% Time: 487.6003s\n",
            "#83 Loss: 0.0052 Acc: 99.5995% Time: 493.3764s\n",
            "#84 Loss: 0.0038 Acc: 100.0000% Time: 499.0243s\n",
            "#85 Loss: 0.0024 Acc: 100.0000% Time: 504.7820s\n",
            "#86 Loss: 0.0021 Acc: 99.8665% Time: 510.2959s\n",
            "#87 Loss: 0.0026 Acc: 100.0000% Time: 515.8187s\n",
            "#88 Loss: 0.0015 Acc: 100.0000% Time: 521.3709s\n",
            "#89 Loss: 0.0034 Acc: 99.7330% Time: 527.0343s\n",
            "#90 Loss: 0.0014 Acc: 100.0000% Time: 532.6946s\n",
            "#91 Loss: 0.0009 Acc: 100.0000% Time: 538.3893s\n",
            "#92 Loss: 0.0018 Acc: 100.0000% Time: 544.2405s\n",
            "#93 Loss: 0.0013 Acc: 100.0000% Time: 549.9251s\n",
            "#94 Loss: 0.0028 Acc: 99.8665% Time: 555.8432s\n",
            "#95 Loss: 0.0007 Acc: 100.0000% Time: 561.5301s\n",
            "#96 Loss: 0.0034 Acc: 99.8665% Time: 567.2560s\n",
            "#97 Loss: 0.0019 Acc: 100.0000% Time: 573.1750s\n",
            "#98 Loss: 0.0011 Acc: 100.0000% Time: 578.9740s\n",
            "#99 Loss: 0.0004 Acc: 100.0000% Time: 584.7993s\n",
            "#100 Loss: 0.0005 Acc: 100.0000% Time: 590.5161s\n",
            "#101 Loss: 0.0071 Acc: 99.7330% Time: 596.3818s\n",
            "#102 Loss: 0.0016 Acc: 100.0000% Time: 602.1361s\n",
            "#103 Loss: 0.0008 Acc: 100.0000% Time: 608.1119s\n",
            "#104 Loss: 0.0006 Acc: 100.0000% Time: 613.9117s\n",
            "#105 Loss: 0.0009 Acc: 100.0000% Time: 619.6534s\n",
            "#106 Loss: 0.0005 Acc: 100.0000% Time: 625.4719s\n",
            "#107 Loss: 0.0045 Acc: 99.7330% Time: 631.2122s\n",
            "#108 Loss: 0.0019 Acc: 100.0000% Time: 636.9291s\n",
            "#109 Loss: 0.0010 Acc: 100.0000% Time: 642.6920s\n",
            "#110 Loss: 0.0005 Acc: 100.0000% Time: 648.6025s\n",
            "#111 Loss: 0.0015 Acc: 100.0000% Time: 654.2225s\n",
            "#112 Loss: 0.0004 Acc: 100.0000% Time: 660.1586s\n",
            "#113 Loss: 0.0010 Acc: 100.0000% Time: 665.9314s\n",
            "#114 Loss: 0.0003 Acc: 100.0000% Time: 671.5938s\n",
            "#115 Loss: 0.0004 Acc: 100.0000% Time: 677.2496s\n",
            "#116 Loss: 0.0007 Acc: 100.0000% Time: 683.1786s\n",
            "#117 Loss: 0.0006 Acc: 100.0000% Time: 689.2256s\n",
            "#118 Loss: 0.0005 Acc: 100.0000% Time: 695.0621s\n",
            "#119 Loss: 0.0003 Acc: 100.0000% Time: 700.7135s\n",
            "#120 Loss: 0.0004 Acc: 100.0000% Time: 706.4214s\n",
            "#121 Loss: 0.0002 Acc: 100.0000% Time: 712.3584s\n",
            "#122 Loss: 0.0002 Acc: 100.0000% Time: 718.0860s\n",
            "#123 Loss: 0.0011 Acc: 100.0000% Time: 723.9135s\n",
            "#124 Loss: 0.0002 Acc: 100.0000% Time: 729.5836s\n",
            "#125 Loss: 0.0006 Acc: 100.0000% Time: 735.5869s\n",
            "#126 Loss: 0.0002 Acc: 100.0000% Time: 741.3261s\n",
            "#127 Loss: 0.0004 Acc: 100.0000% Time: 747.1710s\n",
            "#128 Loss: 0.0014 Acc: 99.8665% Time: 753.1945s\n",
            "#129 Loss: 0.0002 Acc: 100.0000% Time: 759.5330s\n",
            "#130 Loss: 0.0004 Acc: 100.0000% Time: 765.6134s\n",
            "#131 Loss: 0.0003 Acc: 100.0000% Time: 771.3011s\n",
            "#132 Loss: 0.0004 Acc: 100.0000% Time: 776.8228s\n",
            "#133 Loss: 0.0009 Acc: 100.0000% Time: 782.7264s\n",
            "#134 Loss: 0.0005 Acc: 100.0000% Time: 788.7418s\n",
            "#135 Loss: 0.0004 Acc: 100.0000% Time: 794.4463s\n",
            "#136 Loss: 0.0001 Acc: 100.0000% Time: 800.0460s\n",
            "#137 Loss: 0.0002 Acc: 100.0000% Time: 806.0239s\n",
            "#138 Loss: 0.0002 Acc: 100.0000% Time: 811.6658s\n",
            "#139 Loss: 0.0011 Acc: 100.0000% Time: 817.7658s\n",
            "#140 Loss: 0.0010 Acc: 100.0000% Time: 823.5622s\n",
            "#141 Loss: 0.0004 Acc: 100.0000% Time: 829.4752s\n",
            "#142 Loss: 0.0011 Acc: 100.0000% Time: 835.5356s\n",
            "#143 Loss: 0.0007 Acc: 100.0000% Time: 841.8629s\n",
            "#144 Loss: 0.0005 Acc: 100.0000% Time: 847.9900s\n",
            "#145 Loss: 0.0003 Acc: 100.0000% Time: 853.9649s\n",
            "#146 Loss: 0.0002 Acc: 100.0000% Time: 859.9399s\n",
            "#147 Loss: 0.0002 Acc: 100.0000% Time: 865.7053s\n",
            "#148 Loss: 0.0003 Acc: 100.0000% Time: 871.7250s\n",
            "#149 Loss: 0.0003 Acc: 100.0000% Time: 877.8013s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[Test Phase] Loss: 0.2811 Acc: 94.3548% Time: 1.0711s\n",
            "현재 Best는 epoch : 150 / iteration : 5번째 / Loss : 0.28108396162937305 \n",
            "-------------현재 set_9 6번째 학습-------------\n",
            "#0 Loss: 0.4991 Acc: 76.3685% Time: 5.8332s\n",
            "#1 Loss: 0.2835 Acc: 87.7170% Time: 11.5390s\n",
            "#2 Loss: 0.2139 Acc: 90.7877% Time: 17.2426s\n",
            "#3 Loss: 0.1973 Acc: 93.5915% Time: 23.3345s\n",
            "#4 Loss: 0.1426 Acc: 95.4606% Time: 29.6806s\n",
            "#5 Loss: 0.0771 Acc: 97.7303% Time: 35.4980s\n",
            "#6 Loss: 0.1398 Acc: 94.3925% Time: 41.5640s\n",
            "#7 Loss: 0.0957 Acc: 95.9947% Time: 47.6309s\n",
            "#8 Loss: 0.0445 Acc: 98.3979% Time: 53.4676s\n",
            "#9 Loss: 0.0350 Acc: 98.9319% Time: 59.4705s\n",
            "#10 Loss: 0.0570 Acc: 97.3298% Time: 65.4838s\n",
            "#11 Loss: 0.0787 Acc: 97.9973% Time: 71.4691s\n",
            "#12 Loss: 0.0379 Acc: 98.3979% Time: 77.2424s\n",
            "#13 Loss: 0.0440 Acc: 98.2644% Time: 82.9823s\n",
            "#14 Loss: 0.0352 Acc: 98.6649% Time: 88.8177s\n",
            "#15 Loss: 0.0421 Acc: 98.6649% Time: 94.6570s\n",
            "#16 Loss: 0.0346 Acc: 99.0654% Time: 100.8764s\n",
            "#17 Loss: 0.0251 Acc: 98.9319% Time: 107.0769s\n",
            "#18 Loss: 0.0376 Acc: 98.7984% Time: 112.7361s\n",
            "#19 Loss: 0.0350 Acc: 98.9319% Time: 119.0538s\n",
            "#20 Loss: 0.0162 Acc: 99.4660% Time: 124.8873s\n",
            "#21 Loss: 0.0298 Acc: 98.7984% Time: 130.6072s\n",
            "#22 Loss: 0.0234 Acc: 99.0654% Time: 136.5790s\n",
            "#23 Loss: 0.0262 Acc: 98.9319% Time: 142.5483s\n",
            "#24 Loss: 0.0326 Acc: 99.0654% Time: 148.5889s\n",
            "#25 Loss: 0.0130 Acc: 99.5995% Time: 154.3121s\n",
            "#26 Loss: 0.0381 Acc: 98.5314% Time: 160.1004s\n",
            "#27 Loss: 0.0223 Acc: 99.1989% Time: 166.1364s\n",
            "#28 Loss: 0.0182 Acc: 99.4660% Time: 172.0484s\n",
            "#29 Loss: 0.0102 Acc: 99.5995% Time: 177.9225s\n",
            "#30 Loss: 0.0125 Acc: 99.4660% Time: 183.7108s\n",
            "#31 Loss: 0.0097 Acc: 99.7330% Time: 189.7205s\n",
            "#32 Loss: 0.0091 Acc: 99.8665% Time: 195.6136s\n",
            "#33 Loss: 0.0361 Acc: 98.5314% Time: 201.3573s\n",
            "#34 Loss: 0.0096 Acc: 99.7330% Time: 207.2262s\n",
            "#35 Loss: 0.0172 Acc: 99.0654% Time: 213.0663s\n",
            "#36 Loss: 0.0060 Acc: 99.8665% Time: 219.0338s\n",
            "#37 Loss: 0.0065 Acc: 99.8665% Time: 224.8280s\n",
            "#38 Loss: 0.0085 Acc: 99.7330% Time: 230.8382s\n",
            "#39 Loss: 0.0061 Acc: 100.0000% Time: 236.5247s\n",
            "#40 Loss: 0.0073 Acc: 99.4660% Time: 242.7411s\n",
            "#41 Loss: 0.0084 Acc: 99.8665% Time: 248.6123s\n",
            "#42 Loss: 0.0089 Acc: 99.4660% Time: 254.3508s\n",
            "#43 Loss: 0.0022 Acc: 100.0000% Time: 260.1402s\n",
            "#44 Loss: 0.0218 Acc: 99.3324% Time: 266.2600s\n",
            "#45 Loss: 0.0057 Acc: 99.8665% Time: 272.2332s\n",
            "#46 Loss: 0.0080 Acc: 99.5995% Time: 278.1295s\n",
            "#47 Loss: 0.0049 Acc: 99.8665% Time: 284.2240s\n",
            "#48 Loss: 0.0030 Acc: 99.8665% Time: 290.2700s\n",
            "#49 Loss: 0.0041 Acc: 100.0000% Time: 296.3006s\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2434 Acc: 93.5484% Time: 1.0791s\n",
            "현재 Best는 epoch : 50 / iteration : 6번째 / Loss : 0.24342889199359038 \n",
            "-------------현재 set_9 7번째 학습-------------\n",
            "#0 Loss: 0.4624 Acc: 79.0387% Time: 6.1562s\n",
            "#1 Loss: 0.2891 Acc: 88.2510% Time: 11.8784s\n",
            "#2 Loss: 0.1866 Acc: 92.5234% Time: 17.9352s\n",
            "#3 Loss: 0.1951 Acc: 92.6569% Time: 23.8208s\n",
            "#4 Loss: 0.1725 Acc: 93.5915% Time: 29.6969s\n",
            "#5 Loss: 0.1471 Acc: 94.5260% Time: 35.6027s\n",
            "#6 Loss: 0.1275 Acc: 95.0601% Time: 41.5449s\n",
            "#7 Loss: 0.0945 Acc: 96.3952% Time: 47.4494s\n",
            "#8 Loss: 0.1101 Acc: 96.5287% Time: 53.3059s\n",
            "#9 Loss: 0.1276 Acc: 95.9947% Time: 59.1396s\n",
            "#10 Loss: 0.0507 Acc: 98.3979% Time: 65.2416s\n",
            "#11 Loss: 0.0394 Acc: 98.7984% Time: 71.0229s\n",
            "#12 Loss: 0.0405 Acc: 98.7984% Time: 76.8287s\n",
            "#13 Loss: 0.0277 Acc: 98.7984% Time: 82.8925s\n",
            "#14 Loss: 0.0329 Acc: 99.1989% Time: 88.8899s\n",
            "#15 Loss: 0.0310 Acc: 99.1989% Time: 94.8730s\n",
            "#16 Loss: 0.0409 Acc: 98.7984% Time: 100.9879s\n",
            "#17 Loss: 0.0115 Acc: 99.7330% Time: 106.7514s\n",
            "#18 Loss: 0.0141 Acc: 99.5995% Time: 112.7497s\n",
            "#19 Loss: 0.0201 Acc: 99.1989% Time: 118.7912s\n",
            "#20 Loss: 0.0239 Acc: 99.3324% Time: 124.3176s\n",
            "#21 Loss: 0.0277 Acc: 98.6649% Time: 130.2127s\n",
            "#22 Loss: 0.0131 Acc: 99.7330% Time: 136.0019s\n",
            "#23 Loss: 0.0136 Acc: 99.7330% Time: 141.9961s\n",
            "#24 Loss: 0.0206 Acc: 99.1989% Time: 147.8551s\n",
            "#25 Loss: 0.0171 Acc: 99.3324% Time: 153.6670s\n",
            "#26 Loss: 0.0072 Acc: 100.0000% Time: 159.2718s\n",
            "#27 Loss: 0.0047 Acc: 99.8665% Time: 165.2007s\n",
            "#28 Loss: 0.0067 Acc: 100.0000% Time: 170.9575s\n",
            "#29 Loss: 0.0106 Acc: 99.7330% Time: 177.0736s\n",
            "#30 Loss: 0.0098 Acc: 99.7330% Time: 182.7764s\n",
            "#31 Loss: 0.0050 Acc: 99.8665% Time: 188.7717s\n",
            "#32 Loss: 0.0101 Acc: 99.7330% Time: 194.7879s\n",
            "#33 Loss: 0.0337 Acc: 98.6649% Time: 200.5484s\n",
            "#34 Loss: 0.0135 Acc: 99.1989% Time: 206.2964s\n",
            "#35 Loss: 0.0217 Acc: 99.5995% Time: 211.9766s\n",
            "#36 Loss: 0.0174 Acc: 99.4660% Time: 217.9217s\n",
            "#37 Loss: 0.0350 Acc: 99.1989% Time: 223.7218s\n",
            "#38 Loss: 0.0133 Acc: 99.4660% Time: 229.6001s\n",
            "#39 Loss: 0.0036 Acc: 100.0000% Time: 235.3973s\n",
            "#40 Loss: 0.0446 Acc: 99.0654% Time: 241.3864s\n",
            "#41 Loss: 0.0187 Acc: 99.5995% Time: 247.5758s\n",
            "#42 Loss: 0.0126 Acc: 99.5995% Time: 253.3175s\n",
            "#43 Loss: 0.0080 Acc: 100.0000% Time: 259.0225s\n",
            "#44 Loss: 0.0091 Acc: 99.7330% Time: 264.7897s\n",
            "#45 Loss: 0.0091 Acc: 99.5995% Time: 270.7795s\n",
            "#46 Loss: 0.0059 Acc: 100.0000% Time: 276.5253s\n",
            "#47 Loss: 0.0027 Acc: 100.0000% Time: 282.3181s\n",
            "#48 Loss: 0.0046 Acc: 100.0000% Time: 288.0421s\n",
            "#49 Loss: 0.0032 Acc: 100.0000% Time: 293.9085s\n",
            "#50 Loss: 0.0011 Acc: 100.0000% Time: 299.9145s\n",
            "#51 Loss: 0.0034 Acc: 99.8665% Time: 305.9985s\n",
            "#52 Loss: 0.0148 Acc: 99.4660% Time: 311.7042s\n",
            "#53 Loss: 0.0041 Acc: 100.0000% Time: 317.5820s\n",
            "#54 Loss: 0.0020 Acc: 100.0000% Time: 323.2562s\n",
            "#55 Loss: 0.0127 Acc: 99.7330% Time: 328.9229s\n",
            "#56 Loss: 0.0048 Acc: 99.8665% Time: 334.9617s\n",
            "#57 Loss: 0.0067 Acc: 99.8665% Time: 340.8426s\n",
            "#58 Loss: 0.0042 Acc: 100.0000% Time: 346.6713s\n",
            "#59 Loss: 0.0090 Acc: 99.8665% Time: 352.4890s\n",
            "#60 Loss: 0.0187 Acc: 99.4660% Time: 358.3871s\n",
            "#61 Loss: 0.0023 Acc: 100.0000% Time: 364.4552s\n",
            "#62 Loss: 0.0016 Acc: 100.0000% Time: 370.4527s\n",
            "#63 Loss: 0.0081 Acc: 99.7330% Time: 376.1061s\n",
            "#64 Loss: 0.0109 Acc: 99.7330% Time: 381.9482s\n",
            "#65 Loss: 0.0055 Acc: 99.8665% Time: 387.9122s\n",
            "#66 Loss: 0.0020 Acc: 100.0000% Time: 393.8489s\n",
            "#67 Loss: 0.0045 Acc: 99.8665% Time: 399.5761s\n",
            "#68 Loss: 0.0060 Acc: 99.8665% Time: 405.6879s\n",
            "#69 Loss: 0.0014 Acc: 100.0000% Time: 411.4869s\n",
            "#70 Loss: 0.0028 Acc: 100.0000% Time: 417.4145s\n",
            "#71 Loss: 0.0016 Acc: 100.0000% Time: 423.2353s\n",
            "#72 Loss: 0.0050 Acc: 99.8665% Time: 429.2452s\n",
            "#73 Loss: 0.0086 Acc: 99.8665% Time: 434.9883s\n",
            "#74 Loss: 0.0099 Acc: 99.7330% Time: 441.2065s\n",
            "#75 Loss: 0.0068 Acc: 99.8665% Time: 447.0004s\n",
            "#76 Loss: 0.0050 Acc: 99.8665% Time: 452.8683s\n",
            "#77 Loss: 0.0036 Acc: 100.0000% Time: 458.7103s\n",
            "#78 Loss: 0.0018 Acc: 100.0000% Time: 464.6662s\n",
            "#79 Loss: 0.0014 Acc: 100.0000% Time: 470.4663s\n",
            "#80 Loss: 0.0007 Acc: 100.0000% Time: 476.4867s\n",
            "#81 Loss: 0.0012 Acc: 100.0000% Time: 482.2079s\n",
            "#82 Loss: 0.0013 Acc: 100.0000% Time: 488.3334s\n",
            "#83 Loss: 0.0008 Acc: 100.0000% Time: 493.9502s\n",
            "#84 Loss: 0.0041 Acc: 99.8665% Time: 499.8558s\n",
            "#85 Loss: 0.0009 Acc: 100.0000% Time: 505.6411s\n",
            "#86 Loss: 0.0005 Acc: 100.0000% Time: 511.3842s\n",
            "#87 Loss: 0.0010 Acc: 100.0000% Time: 517.3845s\n",
            "#88 Loss: 0.0004 Acc: 100.0000% Time: 523.3351s\n",
            "#89 Loss: 0.0007 Acc: 100.0000% Time: 529.3056s\n",
            "#90 Loss: 0.0008 Acc: 100.0000% Time: 535.4873s\n",
            "#91 Loss: 0.0017 Acc: 100.0000% Time: 541.3047s\n",
            "#92 Loss: 0.0017 Acc: 99.8665% Time: 547.1052s\n",
            "#93 Loss: 0.0012 Acc: 100.0000% Time: 553.1535s\n",
            "#94 Loss: 0.0005 Acc: 100.0000% Time: 559.1875s\n",
            "#95 Loss: 0.0008 Acc: 100.0000% Time: 565.3227s\n",
            "#96 Loss: 0.0006 Acc: 100.0000% Time: 571.3731s\n",
            "#97 Loss: 0.0006 Acc: 100.0000% Time: 577.4786s\n",
            "#98 Loss: 0.0002 Acc: 100.0000% Time: 583.2474s\n",
            "#99 Loss: 0.0002 Acc: 100.0000% Time: 589.2771s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.1851 Acc: 95.1613% Time: 1.0950s\n",
            "현재 Best는 epoch : 100 / iteration : 7번째 / Loss : 0.18509611624458264 \n",
            "-------------현재 set_9 8번째 학습-------------\n",
            "#0 Loss: 0.5231 Acc: 76.1015% Time: 5.8807s\n",
            "#1 Loss: 0.2566 Acc: 90.1202% Time: 11.6686s\n",
            "#2 Loss: 0.2710 Acc: 88.9186% Time: 17.6268s\n",
            "#3 Loss: 0.1765 Acc: 93.4579% Time: 23.3845s\n",
            "#4 Loss: 0.1276 Acc: 95.0601% Time: 29.1635s\n",
            "#5 Loss: 0.1035 Acc: 96.5287% Time: 35.0069s\n",
            "#6 Loss: 0.1012 Acc: 96.3952% Time: 41.0106s\n",
            "#7 Loss: 0.0718 Acc: 97.3298% Time: 46.9587s\n",
            "#8 Loss: 0.0767 Acc: 97.1963% Time: 52.9191s\n",
            "#9 Loss: 0.0795 Acc: 97.1963% Time: 58.9578s\n",
            "#10 Loss: 0.0584 Acc: 97.9973% Time: 64.6485s\n",
            "#11 Loss: 0.0413 Acc: 98.5314% Time: 70.6230s\n",
            "#12 Loss: 0.0311 Acc: 98.7984% Time: 76.4700s\n",
            "#13 Loss: 0.0355 Acc: 98.9319% Time: 82.5775s\n",
            "#14 Loss: 0.0296 Acc: 99.5995% Time: 88.2432s\n",
            "#15 Loss: 0.0470 Acc: 98.2644% Time: 94.2404s\n",
            "#16 Loss: 0.0341 Acc: 98.7984% Time: 100.0350s\n",
            "#17 Loss: 0.0187 Acc: 99.5995% Time: 105.9787s\n",
            "#18 Loss: 0.0165 Acc: 99.4660% Time: 111.7805s\n",
            "#19 Loss: 0.0277 Acc: 98.7984% Time: 117.6757s\n",
            "#20 Loss: 0.0206 Acc: 99.3324% Time: 123.6263s\n",
            "#21 Loss: 0.0159 Acc: 99.4660% Time: 129.5077s\n",
            "#22 Loss: 0.0359 Acc: 99.0654% Time: 135.4708s\n",
            "#23 Loss: 0.0343 Acc: 98.6649% Time: 141.5737s\n",
            "#24 Loss: 0.0222 Acc: 98.7984% Time: 147.3390s\n",
            "#25 Loss: 0.0137 Acc: 99.8665% Time: 153.1336s\n",
            "#26 Loss: 0.0103 Acc: 99.7330% Time: 159.1070s\n",
            "#27 Loss: 0.0615 Acc: 98.3979% Time: 165.0179s\n",
            "#28 Loss: 0.0206 Acc: 99.4660% Time: 170.7534s\n",
            "#29 Loss: 0.0137 Acc: 99.8665% Time: 176.7675s\n",
            "#30 Loss: 0.0105 Acc: 99.8665% Time: 182.8676s\n",
            "#31 Loss: 0.0115 Acc: 99.8665% Time: 188.9418s\n",
            "#32 Loss: 0.0218 Acc: 99.4660% Time: 194.7922s\n",
            "#33 Loss: 0.0199 Acc: 99.3324% Time: 200.6061s\n",
            "#34 Loss: 0.0092 Acc: 99.7330% Time: 206.3799s\n",
            "#35 Loss: 0.0082 Acc: 99.8665% Time: 212.2152s\n",
            "#36 Loss: 0.0178 Acc: 99.4660% Time: 218.1382s\n",
            "#37 Loss: 0.0116 Acc: 99.7330% Time: 224.1958s\n",
            "#38 Loss: 0.0343 Acc: 99.0654% Time: 230.2894s\n",
            "#39 Loss: 0.0074 Acc: 99.8665% Time: 235.9620s\n",
            "#40 Loss: 0.0082 Acc: 99.7330% Time: 241.6463s\n",
            "#41 Loss: 0.0229 Acc: 99.5995% Time: 247.6604s\n",
            "#42 Loss: 0.0046 Acc: 100.0000% Time: 253.4335s\n",
            "#43 Loss: 0.0014 Acc: 100.0000% Time: 259.4090s\n",
            "#44 Loss: 0.0196 Acc: 99.4660% Time: 265.2604s\n",
            "#45 Loss: 0.0055 Acc: 99.8665% Time: 271.1060s\n",
            "#46 Loss: 0.0060 Acc: 99.8665% Time: 277.0281s\n",
            "#47 Loss: 0.0143 Acc: 99.7330% Time: 282.6596s\n",
            "#48 Loss: 0.0038 Acc: 100.0000% Time: 288.3314s\n",
            "#49 Loss: 0.0084 Acc: 99.7330% Time: 294.3369s\n",
            "#50 Loss: 0.0029 Acc: 100.0000% Time: 300.4435s\n",
            "#51 Loss: 0.0033 Acc: 100.0000% Time: 306.2527s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#52 Loss: 0.0133 Acc: 99.4660% Time: 312.0244s\n",
            "#53 Loss: 0.0064 Acc: 99.7330% Time: 318.0785s\n",
            "#54 Loss: 0.0053 Acc: 99.8665% Time: 323.8759s\n",
            "#55 Loss: 0.0042 Acc: 99.8665% Time: 329.8785s\n",
            "#56 Loss: 0.0053 Acc: 99.8665% Time: 335.7046s\n",
            "#57 Loss: 0.0039 Acc: 99.8665% Time: 341.6317s\n",
            "#58 Loss: 0.0019 Acc: 100.0000% Time: 347.2373s\n",
            "#59 Loss: 0.0093 Acc: 99.8665% Time: 353.1429s\n",
            "#60 Loss: 0.0084 Acc: 99.5995% Time: 358.8557s\n",
            "#61 Loss: 0.0018 Acc: 100.0000% Time: 364.6928s\n",
            "#62 Loss: 0.0018 Acc: 100.0000% Time: 370.3546s\n",
            "#63 Loss: 0.0013 Acc: 100.0000% Time: 376.2552s\n",
            "#64 Loss: 0.0014 Acc: 100.0000% Time: 382.1893s\n",
            "#65 Loss: 0.0014 Acc: 100.0000% Time: 388.1364s\n",
            "#66 Loss: 0.0012 Acc: 100.0000% Time: 394.1049s\n",
            "#67 Loss: 0.0009 Acc: 100.0000% Time: 400.0046s\n",
            "#68 Loss: 0.0072 Acc: 99.8665% Time: 405.8311s\n",
            "#69 Loss: 0.0026 Acc: 100.0000% Time: 411.7634s\n",
            "#70 Loss: 0.0011 Acc: 100.0000% Time: 417.5424s\n",
            "#71 Loss: 0.0006 Acc: 100.0000% Time: 423.4528s\n",
            "#72 Loss: 0.0022 Acc: 99.8665% Time: 429.3655s\n",
            "#73 Loss: 0.0012 Acc: 100.0000% Time: 435.3018s\n",
            "#74 Loss: 0.0008 Acc: 100.0000% Time: 441.2682s\n",
            "#75 Loss: 0.0007 Acc: 100.0000% Time: 447.1874s\n",
            "#76 Loss: 0.0008 Acc: 100.0000% Time: 452.9001s\n",
            "#77 Loss: 0.0061 Acc: 99.7330% Time: 458.8290s\n",
            "#78 Loss: 0.0015 Acc: 100.0000% Time: 464.9251s\n",
            "#79 Loss: 0.0009 Acc: 100.0000% Time: 470.9066s\n",
            "#80 Loss: 0.0005 Acc: 100.0000% Time: 476.8419s\n",
            "#81 Loss: 0.0008 Acc: 100.0000% Time: 482.5833s\n",
            "#82 Loss: 0.0028 Acc: 99.8665% Time: 488.4301s\n",
            "#83 Loss: 0.0015 Acc: 100.0000% Time: 494.3298s\n",
            "#84 Loss: 0.0037 Acc: 99.8665% Time: 500.1038s\n",
            "#85 Loss: 0.0081 Acc: 99.8665% Time: 505.8197s\n",
            "#86 Loss: 0.0024 Acc: 99.8665% Time: 511.7160s\n",
            "#87 Loss: 0.0012 Acc: 100.0000% Time: 517.6499s\n",
            "#88 Loss: 0.0205 Acc: 99.5995% Time: 523.6584s\n",
            "#89 Loss: 0.0110 Acc: 99.7330% Time: 529.5010s\n",
            "#90 Loss: 0.0014 Acc: 100.0000% Time: 535.2940s\n",
            "#91 Loss: 0.0017 Acc: 100.0000% Time: 541.2320s\n",
            "#92 Loss: 0.0019 Acc: 99.8665% Time: 547.0363s\n",
            "#93 Loss: 0.0012 Acc: 100.0000% Time: 552.9333s\n",
            "#94 Loss: 0.0006 Acc: 100.0000% Time: 558.8435s\n",
            "#95 Loss: 0.0056 Acc: 99.7330% Time: 564.6324s\n",
            "#96 Loss: 0.0011 Acc: 100.0000% Time: 570.5251s\n",
            "#97 Loss: 0.0020 Acc: 100.0000% Time: 576.3015s\n",
            "#98 Loss: 0.0007 Acc: 100.0000% Time: 582.0917s\n",
            "#99 Loss: 0.0016 Acc: 100.0000% Time: 588.1129s\n",
            "#100 Loss: 0.0010 Acc: 100.0000% Time: 594.2346s\n",
            "#101 Loss: 0.0011 Acc: 100.0000% Time: 600.2703s\n",
            "#102 Loss: 0.0004 Acc: 100.0000% Time: 606.1388s\n",
            "#103 Loss: 0.0062 Acc: 99.7330% Time: 611.9843s\n",
            "#104 Loss: 0.0011 Acc: 100.0000% Time: 617.8210s\n",
            "#105 Loss: 0.0011 Acc: 100.0000% Time: 623.3555s\n",
            "#106 Loss: 0.0006 Acc: 100.0000% Time: 629.1496s\n",
            "#107 Loss: 0.0004 Acc: 100.0000% Time: 635.0784s\n",
            "#108 Loss: 0.0003 Acc: 100.0000% Time: 640.8167s\n",
            "#109 Loss: 0.0004 Acc: 100.0000% Time: 646.5901s\n",
            "#110 Loss: 0.0006 Acc: 100.0000% Time: 652.5032s\n",
            "#111 Loss: 0.0006 Acc: 100.0000% Time: 658.2598s\n",
            "#112 Loss: 0.0004 Acc: 100.0000% Time: 664.0296s\n",
            "#113 Loss: 0.0007 Acc: 100.0000% Time: 670.1640s\n",
            "#114 Loss: 0.0003 Acc: 100.0000% Time: 676.0748s\n",
            "#115 Loss: 0.0007 Acc: 100.0000% Time: 681.8684s\n",
            "#116 Loss: 0.0006 Acc: 100.0000% Time: 687.5830s\n",
            "#117 Loss: 0.0002 Acc: 100.0000% Time: 693.5294s\n",
            "#118 Loss: 0.0003 Acc: 100.0000% Time: 699.5128s\n",
            "#119 Loss: 0.0003 Acc: 100.0000% Time: 705.3682s\n",
            "#120 Loss: 0.0003 Acc: 100.0000% Time: 711.3641s\n",
            "#121 Loss: 0.0002 Acc: 100.0000% Time: 717.0321s\n",
            "#122 Loss: 0.0004 Acc: 100.0000% Time: 722.8403s\n",
            "#123 Loss: 0.0002 Acc: 100.0000% Time: 728.5522s\n",
            "#124 Loss: 0.0006 Acc: 100.0000% Time: 734.7545s\n",
            "#125 Loss: 0.0025 Acc: 99.8665% Time: 740.4695s\n",
            "#126 Loss: 0.0012 Acc: 100.0000% Time: 746.2152s\n",
            "#127 Loss: 0.0002 Acc: 100.0000% Time: 752.0309s\n",
            "#128 Loss: 0.0006 Acc: 100.0000% Time: 758.0901s\n",
            "#129 Loss: 0.0006 Acc: 100.0000% Time: 763.8431s\n",
            "#130 Loss: 0.0005 Acc: 100.0000% Time: 769.6651s\n",
            "#131 Loss: 0.0004 Acc: 100.0000% Time: 775.5959s\n",
            "#132 Loss: 0.0006 Acc: 100.0000% Time: 781.4456s\n",
            "#133 Loss: 0.0003 Acc: 100.0000% Time: 787.3357s\n",
            "#134 Loss: 0.0002 Acc: 100.0000% Time: 793.1803s\n",
            "#135 Loss: 0.0004 Acc: 100.0000% Time: 799.3133s\n",
            "#136 Loss: 0.0005 Acc: 100.0000% Time: 805.2645s\n",
            "#137 Loss: 0.0004 Acc: 100.0000% Time: 811.1305s\n",
            "#138 Loss: 0.0021 Acc: 99.8665% Time: 817.4457s\n",
            "#139 Loss: 0.0007 Acc: 100.0000% Time: 823.2851s\n",
            "#140 Loss: 0.0004 Acc: 100.0000% Time: 829.1170s\n",
            "#141 Loss: 0.0002 Acc: 100.0000% Time: 835.0615s\n",
            "#142 Loss: 0.0133 Acc: 99.3324% Time: 841.1355s\n",
            "#143 Loss: 0.0011 Acc: 100.0000% Time: 847.3033s\n",
            "#144 Loss: 0.0005 Acc: 100.0000% Time: 853.1334s\n",
            "#145 Loss: 0.0019 Acc: 99.8665% Time: 859.0934s\n",
            "#146 Loss: 0.0005 Acc: 100.0000% Time: 864.8229s\n",
            "#147 Loss: 0.0008 Acc: 100.0000% Time: 870.6679s\n",
            "#148 Loss: 0.0004 Acc: 100.0000% Time: 876.6649s\n",
            "#149 Loss: 0.0056 Acc: 99.7330% Time: 882.3695s\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[Test Phase] Loss: 0.2072 Acc: 91.9355% Time: 1.0528s\n",
            "현재 Best는 epoch : 150 / iteration : 8번째 / Loss : 0.20722513832820808 \n",
            "-------------현재 set_9 9번째 학습-------------\n",
            "#0 Loss: 0.5308 Acc: 74.7664% Time: 5.8528s\n",
            "#1 Loss: 0.2851 Acc: 88.6515% Time: 11.7183s\n",
            "#2 Loss: 0.1998 Acc: 91.4553% Time: 17.5927s\n",
            "#3 Loss: 0.1640 Acc: 93.1909% Time: 23.7598s\n",
            "#4 Loss: 0.1962 Acc: 92.5234% Time: 29.8146s\n",
            "#5 Loss: 0.1366 Acc: 94.7931% Time: 35.8618s\n",
            "#6 Loss: 0.1125 Acc: 95.8612% Time: 41.5173s\n",
            "#7 Loss: 0.0609 Acc: 97.5968% Time: 47.3113s\n",
            "#8 Loss: 0.0819 Acc: 96.6622% Time: 53.4335s\n",
            "#9 Loss: 0.0668 Acc: 97.4633% Time: 59.6444s\n",
            "#10 Loss: 0.0549 Acc: 98.1308% Time: 65.7226s\n",
            "#11 Loss: 0.0557 Acc: 97.8638% Time: 71.4655s\n",
            "#12 Loss: 0.0351 Acc: 99.1989% Time: 77.5518s\n",
            "#13 Loss: 0.0264 Acc: 99.0654% Time: 83.4315s\n",
            "#14 Loss: 0.0474 Acc: 98.2644% Time: 89.5210s\n",
            "#15 Loss: 0.0329 Acc: 99.0654% Time: 95.3216s\n",
            "#16 Loss: 0.0291 Acc: 99.1989% Time: 101.3319s\n",
            "#17 Loss: 0.0577 Acc: 97.7303% Time: 107.2767s\n",
            "#18 Loss: 0.0159 Acc: 99.4660% Time: 113.3621s\n",
            "#19 Loss: 0.0234 Acc: 99.3324% Time: 119.2747s\n",
            "#20 Loss: 0.0192 Acc: 99.4660% Time: 125.0513s\n",
            "#21 Loss: 0.0129 Acc: 99.7330% Time: 130.9002s\n",
            "#22 Loss: 0.0187 Acc: 99.5995% Time: 136.8891s\n",
            "#23 Loss: 0.0355 Acc: 98.6649% Time: 142.7764s\n",
            "#24 Loss: 0.0337 Acc: 98.6649% Time: 148.8274s\n",
            "#25 Loss: 0.0312 Acc: 99.0654% Time: 154.8780s\n",
            "#26 Loss: 0.0214 Acc: 99.3324% Time: 160.6351s\n",
            "#27 Loss: 0.0117 Acc: 99.8665% Time: 166.7697s\n",
            "#28 Loss: 0.0195 Acc: 98.9319% Time: 172.7628s\n",
            "#29 Loss: 0.0095 Acc: 99.8665% Time: 178.7888s\n",
            "#30 Loss: 0.0110 Acc: 99.7330% Time: 184.8051s\n",
            "#31 Loss: 0.0048 Acc: 99.8665% Time: 190.5568s\n",
            "#32 Loss: 0.0063 Acc: 100.0000% Time: 196.4520s\n",
            "#33 Loss: 0.0024 Acc: 100.0000% Time: 202.1852s\n",
            "#34 Loss: 0.0041 Acc: 100.0000% Time: 208.1445s\n",
            "#35 Loss: 0.0190 Acc: 99.4660% Time: 214.0789s\n",
            "#36 Loss: 0.0058 Acc: 99.8665% Time: 220.4015s\n",
            "#37 Loss: 0.0366 Acc: 98.9319% Time: 226.4043s\n",
            "#38 Loss: 0.0211 Acc: 99.1989% Time: 232.2101s\n",
            "#39 Loss: 0.0101 Acc: 99.7330% Time: 238.2642s\n",
            "#40 Loss: 0.0048 Acc: 100.0000% Time: 244.2059s\n",
            "#41 Loss: 0.0153 Acc: 99.8665% Time: 250.1099s\n",
            "#42 Loss: 0.0087 Acc: 99.7330% Time: 255.9435s\n",
            "#43 Loss: 0.0104 Acc: 99.5995% Time: 261.8276s\n",
            "#44 Loss: 0.0148 Acc: 99.4660% Time: 267.6939s\n",
            "#45 Loss: 0.0126 Acc: 99.5995% Time: 273.5574s\n",
            "#46 Loss: 0.0035 Acc: 100.0000% Time: 279.6033s\n",
            "#47 Loss: 0.0058 Acc: 99.8665% Time: 285.3811s\n",
            "#48 Loss: 0.0035 Acc: 100.0000% Time: 291.0105s\n",
            "#49 Loss: 0.0063 Acc: 99.8665% Time: 296.9015s\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.1858 Acc: 95.5645% Time: 1.0316s\n",
            "현재 Best는 epoch : 50 / iteration : 9번째 / Loss : 0.1858182937890712 \n"
          ]
        }
      ],
      "source": [
        "epochs = [50, 100, 150]\n",
        "for i in range(9, 10):\n",
        "    # set 9에서 j 번 째 학습을 의미\n",
        "    for j in range(1, 10):\n",
        "        # 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
        "        # 이미지의 밝기(brightness), 대비(contrast), 채도(saturation), 색조(hue)를 일부 변경\n",
        "        transforms_train = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation)\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
        "        ])\n",
        "\n",
        "        transforms_test = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        data_dir = '/home/tutor/안재형/dog_pic/닥스훈트'\n",
        "        data_folder = os.path.join(data_dir, f'set_{i}')\n",
        "        train_datasets = datasets.ImageFolder(os.path.join(data_folder, 'train'), transforms_train)\n",
        "        test_datasets = datasets.ImageFolder(os.path.join(data_folder, 'test'), transforms_test)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=10, shuffle=True, num_workers=2)\n",
        "        test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=10, shuffle=True, num_workers=2)\n",
        "\n",
        "#         print('학습 데이터셋 크기:', len(train_datasets))\n",
        "#         print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "        class_names = train_datasets.classes\n",
        "#         print('클래스:', class_names)\n",
        "\n",
        "        #--------------------------------------------#\n",
        "\n",
        "        model = models.resnet34(pretrained=True)\n",
        "        num_features = model.fc.in_features\n",
        "        # 전이 학습(transfer learning): 모델의 출력 뉴런 수를 2개로 교체하여 마지막 레이어 다시 학습\n",
        "        model.fc = nn.Linear(num_features, 2)\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "        #--------------------------------------------#\n",
        "        print(f'-------------현재 set_{i} {j}번째 학습-------------')\n",
        "        num_epochs = epochs[j % 3]  # 1,2,0 , ...\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 전체 반복(epoch) 수 만큼 반복하며\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0\n",
        "\n",
        "            # 배치 단위로 학습 데이터 불러오기\n",
        "            for inputs, labels in train_dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델에 입력(forward)하고 결과 계산\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(train_datasets)\n",
        "            epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "\n",
        "            # 학습 과정 중에 결과 출력\n",
        "            print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "        #--------------------------------------------#\n",
        "\n",
        "        model.eval()\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in test_dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "                print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "    #             imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "            epoch_loss = running_loss / len(test_datasets)\n",
        "            epoch_acc = running_corrects / len(test_datasets) * 100.\n",
        "            print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "            Loss_Accuracy.append([i, j, epoch_loss, epoch_acc])\n",
        "        \n",
        "        # 8*10개 모든 모델을 저장하지 않고 가장 좋은 성능의 모델을 저장\n",
        "        if j == 1:\n",
        "            Best = [j, epoch_loss, epoch_acc]\n",
        "            # 닥스훈트 set i / j 번째 학습 1인 경우를 의미\n",
        "            torch.save(model, f'dac_set{i}_{j}_1.pth')\n",
        "\n",
        "        elif epoch_loss < Best[2]:\n",
        "            # Best 모델보다 Loss가 개선되면 저장\n",
        "            Best = [j, epoch_loss, epoch_acc]\n",
        "            # 가장 마지막으로 저장된 모델이 제일 좋은 성능\n",
        "            torch.save(model, f'dac_set{i}_{j}_1.pth')\n",
        "        print(f'현재 Best는 epoch : {epoch+1} / iteration : {Best[0]}번째 / Loss : {Best[1]} ')\n",
        "#         print('--------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8381989",
        "outputId": "1b91fe6c-05c3-475f-896f-14b2ff6fad2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "현재 Best는 epoch : 50 / iteration : 9번째 / Loss : 0.1858182937890712 \n"
          ]
        }
      ],
      "source": [
        "print(f'현재 Best는 epoch : {epoch+1} / iteration : {Best[0]}번째 / Loss : {Best[1]} ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "e222945e",
        "outputId": "a3c8dcc0-af47-417c-9b79-8eb978ba9323"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[9, 1, 0.1535915126721281, tensor(93.5484, device='cuda:0')],\n",
              " [9, 1, 0.17863983664767968, tensor(93.9516, device='cuda:0')],\n",
              " [9, 2, 0.2933852207833326, tensor(91.1290, device='cuda:0')],\n",
              " [9, 3, 0.20758715865574026, tensor(94.3548, device='cuda:0')],\n",
              " [9, 4, 0.20669178503287125, tensor(93.9516, device='cuda:0')],\n",
              " [9, 5, 0.28108396162937305, tensor(94.3548, device='cuda:0')],\n",
              " [9, 6, 0.24342889199359038, tensor(93.5484, device='cuda:0')],\n",
              " [9, 7, 0.18509611624458264, tensor(95.1613, device='cuda:0')],\n",
              " [9, 8, 0.20722513832820808, tensor(91.9355, device='cuda:0')],\n",
              " [9, 9, 0.1858182937890712, tensor(95.5645, device='cuda:0')]]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Loss_Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f06e2c1e"
      },
      "source": [
        "- 평균적으로 epoch = 100에서 가장 좋은 성능을 보인다.\n",
        "- 학습 2는 epoch = 100을 기준으로 batch_size를 8 ~ 12까지 확인해보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43b7100b"
      },
      "source": [
        "# 학습 2\n",
        "- epoch = 100\n",
        "- batch_size = range(8, 13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b93d3909"
      },
      "outputs": [],
      "source": [
        "# Loss_Accuracy에 각 배치 사이즈마다의 성능을 넣어둔다\n",
        "Loss_Accuracy = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53859443",
        "outputId": "3ab8971d-6d5d-4968-eb81-acddb09fe91e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------현재 set_9 batch_size는 8-------------\n",
            "#0 Loss: 0.5231 Acc: 76.3685% Time: 6.3759s\n",
            "#1 Loss: 0.3159 Acc: 85.8478% Time: 12.3340s\n",
            "#2 Loss: 0.2646 Acc: 89.4526% Time: 18.2609s\n",
            "#3 Loss: 0.3057 Acc: 88.5180% Time: 24.3093s\n",
            "#4 Loss: 0.1687 Acc: 93.4579% Time: 30.2783s\n",
            "#5 Loss: 0.1248 Acc: 95.7276% Time: 36.3685s\n",
            "#6 Loss: 0.0990 Acc: 97.1963% Time: 42.4939s\n",
            "#7 Loss: 0.1031 Acc: 97.0628% Time: 48.4699s\n",
            "#8 Loss: 0.0816 Acc: 96.5287% Time: 54.4856s\n",
            "#9 Loss: 0.0885 Acc: 96.6622% Time: 60.6731s\n",
            "#10 Loss: 0.0864 Acc: 97.1963% Time: 66.4886s\n",
            "#11 Loss: 0.1304 Acc: 96.3952% Time: 72.3987s\n",
            "#12 Loss: 0.0717 Acc: 97.1963% Time: 78.2490s\n",
            "#13 Loss: 0.0477 Acc: 97.5968% Time: 84.4638s\n",
            "#14 Loss: 0.0277 Acc: 99.1989% Time: 90.5158s\n",
            "#15 Loss: 0.0501 Acc: 97.9973% Time: 96.7152s\n",
            "#16 Loss: 0.0451 Acc: 98.3979% Time: 102.7762s\n",
            "#17 Loss: 0.0359 Acc: 98.7984% Time: 108.7042s\n",
            "#18 Loss: 0.0336 Acc: 98.9319% Time: 114.5620s\n",
            "#19 Loss: 0.0171 Acc: 99.5995% Time: 120.3255s\n",
            "#20 Loss: 0.0239 Acc: 99.3324% Time: 126.3486s\n",
            "#21 Loss: 0.0208 Acc: 99.0654% Time: 132.3652s\n",
            "#22 Loss: 0.0154 Acc: 99.8665% Time: 138.4029s\n",
            "#23 Loss: 0.0240 Acc: 99.4660% Time: 144.2449s\n",
            "#24 Loss: 0.0097 Acc: 99.8665% Time: 150.4419s\n",
            "#25 Loss: 0.0348 Acc: 99.1989% Time: 156.5182s\n",
            "#26 Loss: 0.0430 Acc: 98.5314% Time: 162.4425s\n",
            "#27 Loss: 0.0238 Acc: 99.0654% Time: 168.6031s\n",
            "#28 Loss: 0.0153 Acc: 99.5995% Time: 174.5835s\n",
            "#29 Loss: 0.0248 Acc: 98.9319% Time: 180.5983s\n",
            "#30 Loss: 0.0204 Acc: 99.4660% Time: 186.5736s\n",
            "#31 Loss: 0.0076 Acc: 99.8665% Time: 192.6557s\n",
            "#32 Loss: 0.0078 Acc: 99.7330% Time: 198.6281s\n",
            "#33 Loss: 0.0081 Acc: 99.8665% Time: 204.6402s\n",
            "#34 Loss: 0.0177 Acc: 99.4660% Time: 210.9910s\n",
            "#35 Loss: 0.0264 Acc: 99.1989% Time: 216.9387s\n",
            "#36 Loss: 0.0367 Acc: 98.7984% Time: 223.0664s\n",
            "#37 Loss: 0.0185 Acc: 99.3324% Time: 229.0893s\n",
            "#38 Loss: 0.0291 Acc: 99.0654% Time: 235.1467s\n",
            "#39 Loss: 0.0282 Acc: 99.1989% Time: 241.4901s\n",
            "#40 Loss: 0.0289 Acc: 99.0654% Time: 247.7082s\n",
            "#41 Loss: 0.0169 Acc: 99.7330% Time: 253.7588s\n",
            "#42 Loss: 0.0100 Acc: 99.5995% Time: 259.8376s\n",
            "#43 Loss: 0.0060 Acc: 99.8665% Time: 265.9439s\n",
            "#44 Loss: 0.0075 Acc: 99.8665% Time: 272.0849s\n",
            "#45 Loss: 0.0193 Acc: 99.5995% Time: 278.4416s\n",
            "#46 Loss: 0.0045 Acc: 99.8665% Time: 284.4691s\n",
            "#47 Loss: 0.0047 Acc: 100.0000% Time: 290.6283s\n",
            "#48 Loss: 0.0023 Acc: 100.0000% Time: 296.7253s\n",
            "#49 Loss: 0.0177 Acc: 99.3324% Time: 302.8402s\n",
            "#50 Loss: 0.0044 Acc: 99.8665% Time: 308.8983s\n",
            "#51 Loss: 0.0086 Acc: 99.7330% Time: 315.0950s\n",
            "#52 Loss: 0.0032 Acc: 99.8665% Time: 321.2832s\n",
            "#53 Loss: 0.0022 Acc: 100.0000% Time: 327.3129s\n",
            "#54 Loss: 0.0024 Acc: 100.0000% Time: 333.6394s\n",
            "#55 Loss: 0.0026 Acc: 100.0000% Time: 340.1771s\n",
            "#56 Loss: 0.0029 Acc: 99.8665% Time: 346.1849s\n",
            "#57 Loss: 0.0067 Acc: 99.7330% Time: 352.4316s\n",
            "#58 Loss: 0.0950 Acc: 98.1308% Time: 358.5961s\n",
            "#59 Loss: 0.0273 Acc: 98.9319% Time: 364.6803s\n",
            "#60 Loss: 0.0078 Acc: 99.7330% Time: 370.8160s\n",
            "#61 Loss: 0.0041 Acc: 100.0000% Time: 376.9824s\n",
            "#62 Loss: 0.0058 Acc: 99.7330% Time: 383.3952s\n",
            "#63 Loss: 0.0053 Acc: 99.8665% Time: 389.5537s\n",
            "#64 Loss: 0.0021 Acc: 100.0000% Time: 395.6183s\n",
            "#65 Loss: 0.0014 Acc: 100.0000% Time: 401.7686s\n",
            "#66 Loss: 0.0027 Acc: 100.0000% Time: 407.9393s\n",
            "#67 Loss: 0.0007 Acc: 100.0000% Time: 414.3183s\n",
            "#68 Loss: 0.0008 Acc: 100.0000% Time: 420.4578s\n",
            "#69 Loss: 0.0011 Acc: 100.0000% Time: 426.3698s\n",
            "#70 Loss: 0.0018 Acc: 100.0000% Time: 432.7075s\n",
            "#71 Loss: 0.0009 Acc: 100.0000% Time: 438.9958s\n",
            "#72 Loss: 0.0007 Acc: 100.0000% Time: 445.2673s\n",
            "#73 Loss: 0.0024 Acc: 99.8665% Time: 451.6225s\n",
            "#74 Loss: 0.0024 Acc: 100.0000% Time: 457.8246s\n",
            "#75 Loss: 0.0007 Acc: 100.0000% Time: 464.0469s\n",
            "#76 Loss: 0.0013 Acc: 100.0000% Time: 470.2369s\n",
            "#77 Loss: 0.0010 Acc: 100.0000% Time: 476.3362s\n",
            "#78 Loss: 0.0063 Acc: 99.5995% Time: 482.3940s\n",
            "#79 Loss: 0.0016 Acc: 100.0000% Time: 488.5458s\n",
            "#80 Loss: 0.0006 Acc: 100.0000% Time: 494.6995s\n",
            "#81 Loss: 0.0018 Acc: 99.8665% Time: 501.0917s\n",
            "#82 Loss: 0.0006 Acc: 100.0000% Time: 507.2839s\n",
            "#83 Loss: 0.0190 Acc: 99.1989% Time: 513.6707s\n",
            "#84 Loss: 0.0038 Acc: 99.8665% Time: 519.8658s\n",
            "#85 Loss: 0.0022 Acc: 100.0000% Time: 526.2080s\n",
            "#86 Loss: 0.0011 Acc: 100.0000% Time: 532.2926s\n",
            "#87 Loss: 0.0015 Acc: 100.0000% Time: 538.3270s\n",
            "#88 Loss: 0.0019 Acc: 100.0000% Time: 544.4876s\n",
            "#89 Loss: 0.0007 Acc: 100.0000% Time: 551.0407s\n",
            "#90 Loss: 0.0035 Acc: 99.8665% Time: 557.2802s\n",
            "#91 Loss: 0.0008 Acc: 100.0000% Time: 563.4261s\n",
            "#92 Loss: 0.0007 Acc: 100.0000% Time: 569.6845s\n",
            "#93 Loss: 0.0003 Acc: 100.0000% Time: 575.8722s\n",
            "#94 Loss: 0.0005 Acc: 100.0000% Time: 582.0227s\n",
            "#95 Loss: 0.0007 Acc: 100.0000% Time: 588.1564s\n",
            "#96 Loss: 0.0005 Acc: 100.0000% Time: 594.5781s\n",
            "#97 Loss: 0.0004 Acc: 100.0000% Time: 600.8971s\n",
            "#98 Loss: 0.0003 Acc: 100.0000% Time: 607.0766s\n",
            "#99 Loss: 0.0013 Acc: 99.8665% Time: 613.2384s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2504 Acc: 92.7419% Time: 1.1724s\n",
            "현재 Best는 batch_szie : 8 / Loss : 0.2503840300314257 / Acc : 92.74192810058594\n",
            "-------------현재 set_9 batch_size는 9-------------\n",
            "#0 Loss: 0.4378 Acc: 77.8371% Time: 5.9707s\n",
            "#1 Loss: 0.3034 Acc: 87.4499% Time: 12.0074s\n",
            "#2 Loss: 0.2180 Acc: 90.9212% Time: 18.0160s\n",
            "#3 Loss: 0.1687 Acc: 93.0574% Time: 24.1170s\n",
            "#4 Loss: 0.1169 Acc: 95.8612% Time: 30.1382s\n",
            "#5 Loss: 0.0952 Acc: 96.7957% Time: 36.2968s\n",
            "#6 Loss: 0.1194 Acc: 95.3271% Time: 42.2157s\n",
            "#7 Loss: 0.1151 Acc: 95.5941% Time: 48.0837s\n",
            "#8 Loss: 0.1321 Acc: 95.1936% Time: 54.2356s\n",
            "#9 Loss: 0.0582 Acc: 97.5968% Time: 60.3902s\n",
            "#10 Loss: 0.1038 Acc: 96.3952% Time: 66.4460s\n",
            "#11 Loss: 0.0867 Acc: 96.6622% Time: 72.7059s\n",
            "#12 Loss: 0.0584 Acc: 98.3979% Time: 78.6805s\n",
            "#13 Loss: 0.0412 Acc: 98.1308% Time: 84.7398s\n",
            "#14 Loss: 0.0686 Acc: 97.7303% Time: 90.6349s\n",
            "#15 Loss: 0.0298 Acc: 98.9319% Time: 96.8339s\n",
            "#16 Loss: 0.0304 Acc: 98.9319% Time: 103.0279s\n",
            "#17 Loss: 0.0321 Acc: 98.9319% Time: 109.2521s\n",
            "#18 Loss: 0.0254 Acc: 99.3324% Time: 115.5243s\n",
            "#19 Loss: 0.0171 Acc: 99.4660% Time: 121.6179s\n",
            "#20 Loss: 0.0416 Acc: 98.6649% Time: 127.8074s\n",
            "#21 Loss: 0.0307 Acc: 98.9319% Time: 133.6655s\n",
            "#22 Loss: 0.0275 Acc: 98.9319% Time: 139.6317s\n",
            "#23 Loss: 0.0277 Acc: 99.3324% Time: 145.4848s\n",
            "#24 Loss: 0.0238 Acc: 99.3324% Time: 151.5698s\n",
            "#25 Loss: 0.0075 Acc: 100.0000% Time: 157.7408s\n",
            "#26 Loss: 0.0285 Acc: 98.9319% Time: 163.8738s\n",
            "#27 Loss: 0.0237 Acc: 99.3324% Time: 169.8929s\n",
            "#28 Loss: 0.0081 Acc: 99.7330% Time: 175.8051s\n",
            "#29 Loss: 0.0101 Acc: 99.7330% Time: 181.8364s\n",
            "#30 Loss: 0.1165 Acc: 97.9973% Time: 188.0047s\n",
            "#31 Loss: 0.0522 Acc: 98.7984% Time: 193.9068s\n",
            "#32 Loss: 0.0379 Acc: 98.5314% Time: 200.0971s\n",
            "#33 Loss: 0.0198 Acc: 99.5995% Time: 205.9486s\n",
            "#34 Loss: 0.0207 Acc: 99.5995% Time: 211.8920s\n",
            "#35 Loss: 0.0143 Acc: 99.5995% Time: 217.8606s\n",
            "#36 Loss: 0.0105 Acc: 99.7330% Time: 223.9832s\n",
            "#37 Loss: 0.0071 Acc: 99.7330% Time: 230.1616s\n",
            "#38 Loss: 0.0108 Acc: 99.5995% Time: 236.3584s\n",
            "#39 Loss: 0.0069 Acc: 99.7330% Time: 242.4096s\n",
            "#40 Loss: 0.0415 Acc: 99.0654% Time: 248.8192s\n",
            "#41 Loss: 0.0257 Acc: 99.1989% Time: 255.0473s\n",
            "#42 Loss: 0.0071 Acc: 99.7330% Time: 261.0144s\n",
            "#43 Loss: 0.0034 Acc: 100.0000% Time: 266.9702s\n",
            "#44 Loss: 0.0025 Acc: 100.0000% Time: 272.8209s\n",
            "#45 Loss: 0.0111 Acc: 99.3324% Time: 278.8924s\n",
            "#46 Loss: 0.0049 Acc: 99.8665% Time: 284.7811s\n",
            "#47 Loss: 0.0062 Acc: 99.8665% Time: 290.6304s\n",
            "#48 Loss: 0.0036 Acc: 99.8665% Time: 296.6589s\n",
            "#49 Loss: 0.0100 Acc: 99.4660% Time: 302.8828s\n",
            "#50 Loss: 0.0086 Acc: 99.7330% Time: 309.1682s\n",
            "#51 Loss: 0.0080 Acc: 99.8665% Time: 315.4770s\n",
            "#52 Loss: 0.0060 Acc: 99.7330% Time: 321.6951s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#53 Loss: 0.0096 Acc: 99.8665% Time: 327.7936s\n",
            "#54 Loss: 0.0063 Acc: 99.8665% Time: 333.9800s\n",
            "#55 Loss: 0.0250 Acc: 99.7330% Time: 340.2289s\n",
            "#56 Loss: 0.0200 Acc: 99.5995% Time: 346.2831s\n",
            "#57 Loss: 0.0038 Acc: 99.8665% Time: 352.2505s\n",
            "#58 Loss: 0.0042 Acc: 99.8665% Time: 358.0617s\n",
            "#59 Loss: 0.0023 Acc: 100.0000% Time: 364.2545s\n",
            "#60 Loss: 0.0027 Acc: 99.8665% Time: 370.0769s\n",
            "#61 Loss: 0.0024 Acc: 100.0000% Time: 375.7938s\n",
            "#62 Loss: 0.0022 Acc: 100.0000% Time: 381.9275s\n",
            "#63 Loss: 0.0011 Acc: 100.0000% Time: 387.8766s\n",
            "#64 Loss: 0.0010 Acc: 100.0000% Time: 393.8655s\n",
            "#65 Loss: 0.0007 Acc: 100.0000% Time: 399.9237s\n",
            "#66 Loss: 0.0033 Acc: 99.8665% Time: 406.0604s\n",
            "#67 Loss: 0.0111 Acc: 99.8665% Time: 412.2196s\n",
            "#68 Loss: 0.0023 Acc: 100.0000% Time: 418.2316s\n",
            "#69 Loss: 0.0021 Acc: 100.0000% Time: 424.2495s\n",
            "#70 Loss: 0.0265 Acc: 99.5995% Time: 430.4599s\n",
            "#71 Loss: 0.0157 Acc: 99.1989% Time: 436.4819s\n",
            "#72 Loss: 0.0030 Acc: 99.8665% Time: 442.4706s\n",
            "#73 Loss: 0.0020 Acc: 100.0000% Time: 448.6712s\n",
            "#74 Loss: 0.0023 Acc: 100.0000% Time: 454.7815s\n",
            "#75 Loss: 0.0013 Acc: 100.0000% Time: 460.8599s\n",
            "#76 Loss: 0.0014 Acc: 100.0000% Time: 466.7061s\n",
            "#77 Loss: 0.0009 Acc: 100.0000% Time: 472.9649s\n",
            "#78 Loss: 0.0009 Acc: 100.0000% Time: 479.4534s\n",
            "#79 Loss: 0.0135 Acc: 99.7330% Time: 485.4315s\n",
            "#80 Loss: 0.0037 Acc: 100.0000% Time: 491.8059s\n",
            "#81 Loss: 0.0014 Acc: 100.0000% Time: 497.9227s\n",
            "#82 Loss: 0.0015 Acc: 100.0000% Time: 503.8921s\n",
            "#83 Loss: 0.0026 Acc: 99.8665% Time: 509.7591s\n",
            "#84 Loss: 0.0028 Acc: 99.8665% Time: 515.6341s\n",
            "#85 Loss: 0.0200 Acc: 99.7330% Time: 521.8412s\n",
            "#86 Loss: 0.0223 Acc: 98.9319% Time: 527.8054s\n",
            "#87 Loss: 0.0039 Acc: 99.8665% Time: 533.8033s\n",
            "#88 Loss: 0.0014 Acc: 100.0000% Time: 539.6155s\n",
            "#89 Loss: 0.0098 Acc: 99.5995% Time: 545.7147s\n",
            "#90 Loss: 0.0079 Acc: 99.7330% Time: 551.7930s\n",
            "#91 Loss: 0.0201 Acc: 99.7330% Time: 557.7804s\n",
            "#92 Loss: 0.0222 Acc: 99.4660% Time: 563.7523s\n",
            "#93 Loss: 0.0037 Acc: 100.0000% Time: 569.8006s\n",
            "#94 Loss: 0.0033 Acc: 99.8665% Time: 576.0197s\n",
            "#95 Loss: 0.0022 Acc: 100.0000% Time: 582.1342s\n",
            "#96 Loss: 0.0011 Acc: 100.0000% Time: 588.2059s\n",
            "#97 Loss: 0.0017 Acc: 100.0000% Time: 594.3425s\n",
            "#98 Loss: 0.0009 Acc: 100.0000% Time: 600.5796s\n",
            "#99 Loss: 0.0009 Acc: 100.0000% Time: 606.8354s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.1545 Acc: 94.3548% Time: 1.1196s\n",
            "현재 Best는 batch_szie : 9 / Loss : 0.1545006600478055 / Acc : 94.3548355102539\n",
            "-------------현재 set_9 batch_size는 10-------------\n",
            "#0 Loss: 0.5038 Acc: 75.4339% Time: 6.0559s\n",
            "#1 Loss: 0.3116 Acc: 87.5835% Time: 11.9888s\n",
            "#2 Loss: 0.2021 Acc: 91.9893% Time: 17.7988s\n",
            "#3 Loss: 0.2149 Acc: 91.9893% Time: 23.9781s\n",
            "#4 Loss: 0.1809 Acc: 92.9239% Time: 29.9314s\n",
            "#5 Loss: 0.1226 Acc: 95.4606% Time: 35.8207s\n",
            "#6 Loss: 0.1022 Acc: 96.5287% Time: 41.5169s\n",
            "#7 Loss: 0.0684 Acc: 97.8638% Time: 47.3707s\n",
            "#8 Loss: 0.1136 Acc: 95.7276% Time: 53.2214s\n",
            "#9 Loss: 0.0603 Acc: 97.9973% Time: 59.1078s\n",
            "#10 Loss: 0.0351 Acc: 99.1989% Time: 64.8128s\n",
            "#11 Loss: 0.0464 Acc: 97.9973% Time: 70.6101s\n",
            "#12 Loss: 0.0490 Acc: 98.5314% Time: 76.5831s\n",
            "#13 Loss: 0.0490 Acc: 98.5314% Time: 82.5583s\n",
            "#14 Loss: 0.0230 Acc: 99.4660% Time: 88.4459s\n",
            "#15 Loss: 0.0343 Acc: 98.9319% Time: 94.5474s\n",
            "#16 Loss: 0.0255 Acc: 99.3324% Time: 100.4165s\n",
            "#17 Loss: 0.0262 Acc: 99.0654% Time: 106.3137s\n",
            "#18 Loss: 0.0439 Acc: 98.5314% Time: 112.2827s\n",
            "#19 Loss: 0.0243 Acc: 99.4660% Time: 118.1396s\n",
            "#20 Loss: 0.0479 Acc: 98.7984% Time: 123.7832s\n",
            "#21 Loss: 0.0352 Acc: 98.9319% Time: 129.5205s\n",
            "#22 Loss: 0.0120 Acc: 99.5995% Time: 135.3167s\n",
            "#23 Loss: 0.0163 Acc: 99.4660% Time: 141.3346s\n",
            "#24 Loss: 0.0261 Acc: 99.1989% Time: 146.9683s\n",
            "#25 Loss: 0.0048 Acc: 100.0000% Time: 152.8715s\n",
            "#26 Loss: 0.0036 Acc: 100.0000% Time: 158.6950s\n",
            "#27 Loss: 0.0031 Acc: 100.0000% Time: 164.3959s\n",
            "#28 Loss: 0.0415 Acc: 98.6649% Time: 169.9340s\n",
            "#29 Loss: 0.0275 Acc: 99.3324% Time: 175.8140s\n",
            "#30 Loss: 0.0104 Acc: 99.5995% Time: 181.5106s\n",
            "#31 Loss: 0.0060 Acc: 99.8665% Time: 187.4895s\n",
            "#32 Loss: 0.0170 Acc: 99.4660% Time: 193.5176s\n",
            "#33 Loss: 0.0057 Acc: 99.8665% Time: 199.2774s\n",
            "#34 Loss: 0.0115 Acc: 99.5995% Time: 205.2368s\n",
            "#35 Loss: 0.0195 Acc: 99.5995% Time: 211.1123s\n",
            "#36 Loss: 0.0189 Acc: 99.1989% Time: 216.8486s\n",
            "#37 Loss: 0.0071 Acc: 99.8665% Time: 222.6245s\n",
            "#38 Loss: 0.0099 Acc: 99.5995% Time: 228.3716s\n",
            "#39 Loss: 0.0034 Acc: 100.0000% Time: 234.2762s\n",
            "#40 Loss: 0.0170 Acc: 99.7330% Time: 240.0504s\n",
            "#41 Loss: 0.0180 Acc: 99.4660% Time: 245.8565s\n",
            "#42 Loss: 0.0035 Acc: 100.0000% Time: 251.4024s\n",
            "#43 Loss: 0.0039 Acc: 99.8665% Time: 257.4224s\n",
            "#44 Loss: 0.0156 Acc: 99.4660% Time: 263.2591s\n",
            "#45 Loss: 0.0052 Acc: 99.8665% Time: 269.0303s\n",
            "#46 Loss: 0.0094 Acc: 99.7330% Time: 274.7849s\n",
            "#47 Loss: 0.0135 Acc: 99.5995% Time: 280.6205s\n",
            "#48 Loss: 0.0029 Acc: 100.0000% Time: 286.2295s\n",
            "#49 Loss: 0.0028 Acc: 100.0000% Time: 291.7136s\n",
            "#50 Loss: 0.0114 Acc: 99.4660% Time: 297.4484s\n",
            "#51 Loss: 0.0057 Acc: 99.8665% Time: 303.2353s\n",
            "#52 Loss: 0.0044 Acc: 99.8665% Time: 308.8035s\n",
            "#53 Loss: 0.0104 Acc: 99.8665% Time: 314.3075s\n",
            "#54 Loss: 0.0055 Acc: 99.8665% Time: 320.1148s\n",
            "#55 Loss: 0.0081 Acc: 99.8665% Time: 325.8391s\n",
            "#56 Loss: 0.0041 Acc: 100.0000% Time: 331.6648s\n",
            "#57 Loss: 0.0020 Acc: 100.0000% Time: 337.2996s\n",
            "#58 Loss: 0.0117 Acc: 99.5995% Time: 343.2870s\n",
            "#59 Loss: 0.0023 Acc: 100.0000% Time: 349.1219s\n",
            "#60 Loss: 0.0023 Acc: 100.0000% Time: 355.1848s\n",
            "#61 Loss: 0.0021 Acc: 100.0000% Time: 361.0257s\n",
            "#62 Loss: 0.0013 Acc: 100.0000% Time: 366.8030s\n",
            "#63 Loss: 0.0010 Acc: 100.0000% Time: 372.5993s\n",
            "#64 Loss: 0.0010 Acc: 100.0000% Time: 378.2543s\n",
            "#65 Loss: 0.0048 Acc: 99.8665% Time: 383.6930s\n",
            "#66 Loss: 0.0015 Acc: 100.0000% Time: 389.7402s\n",
            "#67 Loss: 0.0012 Acc: 100.0000% Time: 395.2378s\n",
            "#68 Loss: 0.0009 Acc: 100.0000% Time: 400.9361s\n",
            "#69 Loss: 0.0017 Acc: 99.8665% Time: 406.7758s\n",
            "#70 Loss: 0.0010 Acc: 100.0000% Time: 412.5762s\n",
            "#71 Loss: 0.0011 Acc: 100.0000% Time: 418.4910s\n",
            "#72 Loss: 0.0011 Acc: 100.0000% Time: 424.2441s\n",
            "#73 Loss: 0.0008 Acc: 100.0000% Time: 430.2435s\n",
            "#74 Loss: 0.0005 Acc: 100.0000% Time: 436.0409s\n",
            "#75 Loss: 0.0013 Acc: 100.0000% Time: 441.8351s\n",
            "#76 Loss: 0.0005 Acc: 100.0000% Time: 447.5047s\n",
            "#77 Loss: 0.0006 Acc: 100.0000% Time: 453.1080s\n",
            "#78 Loss: 0.0006 Acc: 100.0000% Time: 458.7518s\n",
            "#79 Loss: 0.0015 Acc: 100.0000% Time: 464.7850s\n",
            "#80 Loss: 0.0010 Acc: 100.0000% Time: 470.2726s\n",
            "#81 Loss: 0.0012 Acc: 100.0000% Time: 476.0646s\n",
            "#82 Loss: 0.0039 Acc: 99.8665% Time: 481.8130s\n",
            "#83 Loss: 0.0016 Acc: 100.0000% Time: 487.6386s\n",
            "#84 Loss: 0.0041 Acc: 99.8665% Time: 493.5429s\n",
            "#85 Loss: 0.0066 Acc: 99.7330% Time: 499.1333s\n",
            "#86 Loss: 0.0038 Acc: 99.8665% Time: 504.6914s\n",
            "#87 Loss: 0.0045 Acc: 99.8665% Time: 510.1951s\n",
            "#88 Loss: 0.0026 Acc: 100.0000% Time: 515.9239s\n",
            "#89 Loss: 0.0072 Acc: 99.7330% Time: 521.9428s\n",
            "#90 Loss: 0.0022 Acc: 100.0000% Time: 527.8086s\n",
            "#91 Loss: 0.0008 Acc: 100.0000% Time: 533.5509s\n",
            "#92 Loss: 0.0006 Acc: 100.0000% Time: 539.2606s\n",
            "#93 Loss: 0.0016 Acc: 100.0000% Time: 545.2573s\n",
            "#94 Loss: 0.0004 Acc: 100.0000% Time: 551.0051s\n",
            "#95 Loss: 0.0007 Acc: 100.0000% Time: 556.9353s\n",
            "#96 Loss: 0.0026 Acc: 99.8665% Time: 562.5712s\n",
            "#97 Loss: 0.0034 Acc: 99.8665% Time: 568.2233s\n",
            "#98 Loss: 0.0017 Acc: 100.0000% Time: 573.9270s\n",
            "#99 Loss: 0.0047 Acc: 99.8665% Time: 579.5509s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[Test Phase] Loss: 0.2523 Acc: 92.3387% Time: 1.1333s\n",
            "현재 Best는 batch_szie : 9 / Loss : 0.1545006600478055 / Acc : 94.3548355102539\n",
            "-------------현재 set_9 batch_size는 11-------------\n",
            "#0 Loss: 0.4547 Acc: 79.9733% Time: 5.7947s\n",
            "#1 Loss: 0.3139 Acc: 85.8478% Time: 11.7417s\n",
            "#2 Loss: 0.2425 Acc: 90.3872% Time: 17.5350s\n",
            "#3 Loss: 0.1707 Acc: 94.1255% Time: 23.4073s\n",
            "#4 Loss: 0.1349 Acc: 94.9266% Time: 29.3323s\n",
            "#5 Loss: 0.1496 Acc: 94.2590% Time: 35.3591s\n",
            "#6 Loss: 0.1204 Acc: 95.7276% Time: 41.2723s\n",
            "#7 Loss: 0.0843 Acc: 97.0628% Time: 46.8406s\n",
            "#8 Loss: 0.0940 Acc: 96.5287% Time: 52.5882s\n",
            "#9 Loss: 0.1527 Acc: 94.3925% Time: 58.3613s\n",
            "#10 Loss: 0.0876 Acc: 95.7276% Time: 64.0832s\n",
            "#11 Loss: 0.0349 Acc: 98.6649% Time: 69.5951s\n",
            "#12 Loss: 0.0525 Acc: 98.1308% Time: 75.2760s\n",
            "#13 Loss: 0.0422 Acc: 98.7984% Time: 80.8807s\n",
            "#14 Loss: 0.0552 Acc: 98.2644% Time: 86.4920s\n",
            "#15 Loss: 0.0383 Acc: 99.0654% Time: 91.9095s\n",
            "#16 Loss: 0.0434 Acc: 98.6649% Time: 97.6474s\n",
            "#17 Loss: 0.0345 Acc: 98.9319% Time: 103.5296s\n",
            "#18 Loss: 0.0653 Acc: 97.7303% Time: 109.2586s\n",
            "#19 Loss: 0.0389 Acc: 98.3979% Time: 115.4269s\n",
            "#20 Loss: 0.0202 Acc: 99.3324% Time: 121.0137s\n",
            "#21 Loss: 0.0495 Acc: 97.8638% Time: 126.8110s\n",
            "#22 Loss: 0.0152 Acc: 99.4660% Time: 132.6275s\n",
            "#23 Loss: 0.0195 Acc: 99.1989% Time: 138.3928s\n",
            "#24 Loss: 0.0187 Acc: 99.3324% Time: 144.1808s\n",
            "#25 Loss: 0.0243 Acc: 98.9319% Time: 149.9357s\n",
            "#26 Loss: 0.0295 Acc: 98.7984% Time: 155.9116s\n",
            "#27 Loss: 0.0231 Acc: 99.1989% Time: 161.9856s\n",
            "#28 Loss: 0.0416 Acc: 98.6649% Time: 167.7488s\n",
            "#29 Loss: 0.0259 Acc: 99.0654% Time: 173.3687s\n",
            "#30 Loss: 0.0217 Acc: 99.1989% Time: 179.0492s\n",
            "#31 Loss: 0.0358 Acc: 98.7984% Time: 184.7800s\n",
            "#32 Loss: 0.0203 Acc: 99.0654% Time: 190.4559s\n",
            "#33 Loss: 0.0227 Acc: 99.0654% Time: 196.5895s\n",
            "#34 Loss: 0.0555 Acc: 97.7303% Time: 202.4354s\n",
            "#35 Loss: 0.0523 Acc: 97.8638% Time: 208.2409s\n",
            "#36 Loss: 0.0544 Acc: 98.2644% Time: 213.9697s\n",
            "#37 Loss: 0.0254 Acc: 99.0654% Time: 219.7782s\n",
            "#38 Loss: 0.0098 Acc: 99.7330% Time: 225.7986s\n",
            "#39 Loss: 0.0175 Acc: 99.0654% Time: 231.5487s\n",
            "#40 Loss: 0.0171 Acc: 99.1989% Time: 237.5795s\n",
            "#41 Loss: 0.0091 Acc: 99.7330% Time: 243.2596s\n",
            "#42 Loss: 0.0078 Acc: 99.8665% Time: 248.9498s\n",
            "#43 Loss: 0.0090 Acc: 99.8665% Time: 254.6463s\n",
            "#44 Loss: 0.0127 Acc: 99.5995% Time: 260.2716s\n",
            "#45 Loss: 0.0061 Acc: 99.8665% Time: 266.0332s\n",
            "#46 Loss: 0.0147 Acc: 99.5995% Time: 271.9851s\n",
            "#47 Loss: 0.0199 Acc: 99.3324% Time: 277.9458s\n",
            "#48 Loss: 0.0035 Acc: 100.0000% Time: 283.7789s\n",
            "#49 Loss: 0.0030 Acc: 100.0000% Time: 289.5441s\n",
            "#50 Loss: 0.0034 Acc: 100.0000% Time: 295.2727s\n",
            "#51 Loss: 0.0081 Acc: 99.5995% Time: 301.0708s\n",
            "#52 Loss: 0.0053 Acc: 99.8665% Time: 306.8179s\n",
            "#53 Loss: 0.0042 Acc: 99.8665% Time: 312.8172s\n",
            "#54 Loss: 0.0059 Acc: 99.7330% Time: 318.8324s\n",
            "#55 Loss: 0.0045 Acc: 100.0000% Time: 324.5807s\n",
            "#56 Loss: 0.0020 Acc: 100.0000% Time: 330.4819s\n",
            "#57 Loss: 0.0018 Acc: 100.0000% Time: 336.1672s\n",
            "#58 Loss: 0.0015 Acc: 100.0000% Time: 341.7501s\n",
            "#59 Loss: 0.0146 Acc: 99.7330% Time: 347.3571s\n",
            "#60 Loss: 0.0349 Acc: 99.0654% Time: 353.0909s\n",
            "#61 Loss: 0.0289 Acc: 98.9319% Time: 358.7582s\n",
            "#62 Loss: 0.0048 Acc: 99.7330% Time: 364.4577s\n",
            "#63 Loss: 0.0088 Acc: 99.7330% Time: 370.0627s\n",
            "#64 Loss: 0.0083 Acc: 99.5995% Time: 375.7451s\n",
            "#65 Loss: 0.0017 Acc: 100.0000% Time: 381.2104s\n",
            "#66 Loss: 0.0099 Acc: 99.8665% Time: 387.3137s\n",
            "#67 Loss: 0.0465 Acc: 98.2644% Time: 393.1531s\n",
            "#68 Loss: 0.0044 Acc: 99.8665% Time: 399.0188s\n",
            "#69 Loss: 0.0116 Acc: 99.8665% Time: 405.0057s\n",
            "#70 Loss: 0.0345 Acc: 98.6649% Time: 410.7565s\n",
            "#71 Loss: 0.0363 Acc: 98.7984% Time: 416.7842s\n",
            "#72 Loss: 0.0072 Acc: 99.8665% Time: 422.5866s\n",
            "#73 Loss: 0.0155 Acc: 99.4660% Time: 428.3870s\n",
            "#74 Loss: 0.0304 Acc: 98.9319% Time: 434.3254s\n",
            "#75 Loss: 0.0435 Acc: 98.7984% Time: 440.1685s\n",
            "#76 Loss: 0.0174 Acc: 99.3324% Time: 446.0031s\n",
            "#77 Loss: 0.0143 Acc: 99.7330% Time: 451.8524s\n",
            "#78 Loss: 0.0069 Acc: 99.7330% Time: 457.6411s\n",
            "#79 Loss: 0.0055 Acc: 100.0000% Time: 463.5473s\n",
            "#80 Loss: 0.0198 Acc: 99.1989% Time: 469.4260s\n",
            "#81 Loss: 0.0210 Acc: 99.0654% Time: 475.3597s\n",
            "#82 Loss: 0.0164 Acc: 99.1989% Time: 481.0525s\n",
            "#83 Loss: 0.0191 Acc: 99.5995% Time: 486.7514s\n",
            "#84 Loss: 0.0044 Acc: 99.8665% Time: 492.7193s\n",
            "#85 Loss: 0.0105 Acc: 99.7330% Time: 498.6264s\n",
            "#86 Loss: 0.0235 Acc: 99.1989% Time: 504.6488s\n",
            "#87 Loss: 0.0355 Acc: 98.5314% Time: 510.4615s\n",
            "#88 Loss: 0.0138 Acc: 99.5995% Time: 516.1140s\n",
            "#89 Loss: 0.0087 Acc: 99.7330% Time: 521.7126s\n",
            "#90 Loss: 0.0329 Acc: 98.9319% Time: 527.5413s\n",
            "#91 Loss: 0.0559 Acc: 97.9973% Time: 533.5058s\n",
            "#92 Loss: 0.0195 Acc: 99.1989% Time: 539.1822s\n",
            "#93 Loss: 0.0280 Acc: 98.7984% Time: 545.1279s\n",
            "#94 Loss: 0.1039 Acc: 96.3952% Time: 550.9948s\n",
            "#95 Loss: 0.0084 Acc: 99.5995% Time: 556.7496s\n",
            "#96 Loss: 0.0032 Acc: 99.8665% Time: 562.3861s\n",
            "#97 Loss: 0.0051 Acc: 99.8665% Time: 568.3895s\n",
            "#98 Loss: 0.0049 Acc: 99.8665% Time: 574.2138s\n",
            "#99 Loss: 0.0031 Acc: 99.8665% Time: 579.9655s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2597 Acc: 93.1452% Time: 1.0270s\n",
            "현재 Best는 batch_szie : 9 / Loss : 0.1545006600478055 / Acc : 94.3548355102539\n",
            "-------------현재 set_9 batch_size는 12-------------\n",
            "#0 Loss: 0.4725 Acc: 77.8371% Time: 5.7458s\n",
            "#1 Loss: 0.2932 Acc: 88.2510% Time: 11.2483s\n",
            "#2 Loss: 0.2106 Acc: 91.1883% Time: 16.8855s\n",
            "#3 Loss: 0.1976 Acc: 92.6569% Time: 22.6381s\n",
            "#4 Loss: 0.1696 Acc: 93.8585% Time: 28.7604s\n",
            "#5 Loss: 0.1775 Acc: 92.7904% Time: 34.2593s\n",
            "#6 Loss: 0.0456 Acc: 98.6649% Time: 39.7847s\n",
            "#7 Loss: 0.0882 Acc: 96.7957% Time: 45.5239s\n",
            "#8 Loss: 0.0908 Acc: 96.2617% Time: 50.9991s\n",
            "#9 Loss: 0.0729 Acc: 97.1963% Time: 56.5400s\n",
            "#10 Loss: 0.0435 Acc: 98.6649% Time: 62.3597s\n",
            "#11 Loss: 0.0760 Acc: 96.7957% Time: 68.1427s\n",
            "#12 Loss: 0.0419 Acc: 98.7984% Time: 73.8457s\n",
            "#13 Loss: 0.0350 Acc: 99.1989% Time: 79.5173s\n",
            "#14 Loss: 0.0796 Acc: 97.8638% Time: 85.2261s\n",
            "#15 Loss: 0.0429 Acc: 97.9973% Time: 90.7050s\n",
            "#16 Loss: 0.0158 Acc: 99.5995% Time: 96.5655s\n",
            "#17 Loss: 0.0159 Acc: 99.7330% Time: 102.1814s\n",
            "#18 Loss: 0.0238 Acc: 99.1989% Time: 107.9124s\n",
            "#19 Loss: 0.0182 Acc: 99.5995% Time: 113.5932s\n",
            "#20 Loss: 0.0177 Acc: 99.5995% Time: 119.2359s\n",
            "#21 Loss: 0.0219 Acc: 99.4660% Time: 124.8955s\n",
            "#22 Loss: 0.0163 Acc: 99.4660% Time: 130.5053s\n",
            "#23 Loss: 0.0436 Acc: 98.6649% Time: 136.3140s\n",
            "#24 Loss: 0.0360 Acc: 99.1989% Time: 142.0940s\n",
            "#25 Loss: 0.0259 Acc: 99.3324% Time: 147.7200s\n",
            "#26 Loss: 0.0137 Acc: 99.7330% Time: 153.2701s\n",
            "#27 Loss: 0.0129 Acc: 99.7330% Time: 158.8603s\n",
            "#28 Loss: 0.0235 Acc: 99.3324% Time: 164.8622s\n",
            "#29 Loss: 0.0068 Acc: 99.8665% Time: 170.4767s\n",
            "#30 Loss: 0.0140 Acc: 99.5995% Time: 176.3854s\n",
            "#31 Loss: 0.0079 Acc: 99.8665% Time: 182.1911s\n",
            "#32 Loss: 0.0140 Acc: 99.4660% Time: 187.9085s\n",
            "#33 Loss: 0.0232 Acc: 98.9319% Time: 193.8394s\n",
            "#34 Loss: 0.0124 Acc: 99.7330% Time: 199.4616s\n",
            "#35 Loss: 0.0050 Acc: 99.8665% Time: 205.2045s\n",
            "#36 Loss: 0.0134 Acc: 99.4660% Time: 211.0753s\n",
            "#37 Loss: 0.0039 Acc: 99.8665% Time: 216.7348s\n",
            "#38 Loss: 0.0059 Acc: 99.8665% Time: 222.3415s\n",
            "#39 Loss: 0.0093 Acc: 99.8665% Time: 227.9944s\n",
            "#40 Loss: 0.0122 Acc: 99.4660% Time: 234.2988s\n",
            "#41 Loss: 0.0203 Acc: 99.1989% Time: 239.9547s\n",
            "#42 Loss: 0.0065 Acc: 99.8665% Time: 245.3962s\n",
            "#43 Loss: 0.0228 Acc: 99.4660% Time: 250.8308s\n",
            "#44 Loss: 0.0262 Acc: 98.9319% Time: 256.4963s\n",
            "#45 Loss: 0.0374 Acc: 98.9319% Time: 262.2056s\n",
            "#46 Loss: 0.0101 Acc: 99.7330% Time: 268.1026s\n",
            "#47 Loss: 0.0169 Acc: 99.3324% Time: 273.7213s\n",
            "#48 Loss: 0.0198 Acc: 99.3324% Time: 279.3895s\n",
            "#49 Loss: 0.0127 Acc: 99.5995% Time: 284.9389s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#50 Loss: 0.0106 Acc: 99.7330% Time: 290.9176s\n",
            "#51 Loss: 0.0065 Acc: 99.7330% Time: 296.5084s\n",
            "#52 Loss: 0.0033 Acc: 100.0000% Time: 302.1808s\n",
            "#53 Loss: 0.0041 Acc: 99.8665% Time: 307.7756s\n",
            "#54 Loss: 0.0282 Acc: 99.4660% Time: 313.7295s\n",
            "#55 Loss: 0.0104 Acc: 99.4660% Time: 319.3716s\n",
            "#56 Loss: 0.0135 Acc: 99.5995% Time: 325.0933s\n",
            "#57 Loss: 0.0065 Acc: 99.8665% Time: 331.0007s\n",
            "#58 Loss: 0.0109 Acc: 99.7330% Time: 336.7056s\n",
            "#59 Loss: 0.0230 Acc: 99.3324% Time: 342.3273s\n",
            "#60 Loss: 0.0103 Acc: 99.5995% Time: 348.3972s\n",
            "#61 Loss: 0.0043 Acc: 99.8665% Time: 353.8747s\n",
            "#62 Loss: 0.0052 Acc: 99.8665% Time: 359.5373s\n",
            "#63 Loss: 0.0057 Acc: 99.7330% Time: 365.3521s\n",
            "#64 Loss: 0.0043 Acc: 99.8665% Time: 370.7962s\n",
            "#65 Loss: 0.0120 Acc: 99.7330% Time: 376.4763s\n",
            "#66 Loss: 0.0030 Acc: 100.0000% Time: 382.2632s\n",
            "#67 Loss: 0.0031 Acc: 99.8665% Time: 387.7932s\n",
            "#68 Loss: 0.0023 Acc: 100.0000% Time: 393.6482s\n",
            "#69 Loss: 0.0018 Acc: 100.0000% Time: 399.3520s\n",
            "#70 Loss: 0.0016 Acc: 100.0000% Time: 405.0680s\n",
            "#71 Loss: 0.0014 Acc: 100.0000% Time: 410.6954s\n",
            "#72 Loss: 0.0009 Acc: 100.0000% Time: 416.1783s\n",
            "#73 Loss: 0.0019 Acc: 99.8665% Time: 422.1204s\n",
            "#74 Loss: 0.0031 Acc: 100.0000% Time: 427.8016s\n",
            "#75 Loss: 0.0032 Acc: 100.0000% Time: 433.5274s\n",
            "#76 Loss: 0.0019 Acc: 100.0000% Time: 439.0634s\n",
            "#77 Loss: 0.0013 Acc: 100.0000% Time: 444.8901s\n",
            "#78 Loss: 0.0030 Acc: 99.8665% Time: 450.4665s\n",
            "#79 Loss: 0.0020 Acc: 100.0000% Time: 456.4654s\n",
            "#80 Loss: 0.0010 Acc: 100.0000% Time: 462.2770s\n",
            "#81 Loss: 0.0009 Acc: 100.0000% Time: 468.4798s\n",
            "#82 Loss: 0.0011 Acc: 100.0000% Time: 474.0890s\n",
            "#83 Loss: 0.0078 Acc: 99.8665% Time: 479.7238s\n",
            "#84 Loss: 0.0027 Acc: 99.8665% Time: 485.5326s\n",
            "#85 Loss: 0.0027 Acc: 100.0000% Time: 491.2035s\n",
            "#86 Loss: 0.0018 Acc: 99.8665% Time: 496.8641s\n",
            "#87 Loss: 0.0094 Acc: 99.3324% Time: 502.6079s\n",
            "#88 Loss: 0.0084 Acc: 99.5995% Time: 508.1578s\n",
            "#89 Loss: 0.0187 Acc: 99.0654% Time: 514.1138s\n",
            "#90 Loss: 0.0092 Acc: 99.7330% Time: 519.7695s\n",
            "#91 Loss: 0.0021 Acc: 100.0000% Time: 525.2746s\n",
            "#92 Loss: 0.0030 Acc: 100.0000% Time: 531.0059s\n",
            "#93 Loss: 0.0027 Acc: 100.0000% Time: 536.6260s\n",
            "#94 Loss: 0.0112 Acc: 99.5995% Time: 542.3117s\n",
            "#95 Loss: 0.0105 Acc: 99.7330% Time: 547.8496s\n",
            "#96 Loss: 0.0100 Acc: 99.7330% Time: 553.8447s\n",
            "#97 Loss: 0.0058 Acc: 99.8665% Time: 559.4535s\n",
            "#98 Loss: 0.0059 Acc: 99.8665% Time: 565.1367s\n",
            "#99 Loss: 0.0067 Acc: 99.8665% Time: 571.1024s\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: fat] (실제 정답: fat)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[예측 결과: nor] (실제 정답: nor)\n",
            "[Test Phase] Loss: 0.2400 Acc: 94.3548% Time: 1.0384s\n",
            "현재 Best는 batch_szie : 9 / Loss : 0.1545006600478055 / Acc : 94.3548355102539\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# epochs = [50, 100, 150]\n",
        "for i in range(9, 10):\n",
        "    # set 9에서 j 번 째 학습을 의미\n",
        "    for batch in range(8, 13):\n",
        "        # 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
        "        # 이미지의 밝기(brightness), 대비(contrast), 채도(saturation), 색조(hue)를 일부 변경\n",
        "        transforms_train = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation)\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
        "        ])\n",
        "\n",
        "        transforms_test = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        data_dir = '/home/tutor/안재형/dog_pic/닥스훈트'\n",
        "        data_folder = os.path.join(data_dir, f'set_{i}')\n",
        "        train_datasets = datasets.ImageFolder(os.path.join(data_folder, 'train'), transforms_train)\n",
        "        test_datasets = datasets.ImageFolder(os.path.join(data_folder, 'test'), transforms_test)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=batch, shuffle=True, num_workers=2)\n",
        "        test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=batch, shuffle=True, num_workers=2)\n",
        "\n",
        "#         print('학습 데이터셋 크기:', len(train_datasets))\n",
        "#         print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "        class_names = train_datasets.classes\n",
        "#         print('클래스:', class_names)\n",
        "\n",
        "        #--------------------------------------------#\n",
        "\n",
        "        model = models.resnet34(pretrained=True)\n",
        "        num_features = model.fc.in_features\n",
        "        # 전이 학습(transfer learning): 모델의 출력 뉴런 수를 2개로 교체하여 마지막 레이어 다시 학습\n",
        "        model.fc = nn.Linear(num_features, 2)\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "        #--------------------------------------------#\n",
        "        print(f'-------------현재 set_{i} batch_size는 {batch}-------------')\n",
        "        num_epochs = 100\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 전체 반복(epoch) 수 만큼 반복하며\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0\n",
        "\n",
        "            # 배치 단위로 학습 데이터 불러오기\n",
        "            for inputs, labels in train_dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델에 입력(forward)하고 결과 계산\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(train_datasets)\n",
        "            epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "\n",
        "            # 학습 과정 중에 결과 출력\n",
        "            print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "        #--------------------------------------------#\n",
        "\n",
        "        model.eval()\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in test_dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "                print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "    #             imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "            epoch_loss = running_loss / len(test_datasets)\n",
        "            epoch_acc = running_corrects / len(test_datasets) * 100.\n",
        "            print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "            Loss_Accuracy.append([i, batch, epoch_loss, epoch_acc])\n",
        "        \n",
        "        # 8*10개 모든 모델을 저장하지 않고 가장 좋은 성능의 모델을 저장\n",
        "        if batch == 8:\n",
        "            Best = [batch, epoch_loss, epoch_acc]\n",
        "            # 닥스훈트 set i / batch_size는 batch인 학습 2를 의미\n",
        "            torch.save(model, f'dac_set{i}_b{batch}_2.pth')\n",
        "\n",
        "        elif epoch_loss < Best[1]:\n",
        "            # Best 모델보다 Loss가 개선되면 저장\n",
        "            Best = [batch, epoch_loss, epoch_acc]\n",
        "            # 가장 마지막으로 저장된 모델이 제일 좋은 성능\n",
        "            torch.save(model, f'dac_set{i}_b{batch}_2.pth')\n",
        "        print(f'현재 Best는 batch_szie : {Best[0]} / Loss : {Best[1]} / Acc : {Best[2]}')\n",
        "    print()\n",
        "#         print('--------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13ab422d",
        "outputId": "25907587-e004-41e2-ed67-d256ba1a3fa4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[9, 1, 0.1535915126721281, tensor(93.5484, device='cuda:0')],\n",
              " [9, 1, 0.17863983664767968, tensor(93.9516, device='cuda:0')],\n",
              " [9, 2, 0.2933852207833326, tensor(91.1290, device='cuda:0')],\n",
              " [9, 3, 0.20758715865574026, tensor(94.3548, device='cuda:0')],\n",
              " [9, 4, 0.20669178503287125, tensor(93.9516, device='cuda:0')],\n",
              " [9, 5, 0.28108396162937305, tensor(94.3548, device='cuda:0')],\n",
              " [9, 6, 0.24342889199359038, tensor(93.5484, device='cuda:0')],\n",
              " [9, 7, 0.18509611624458264, tensor(95.1613, device='cuda:0')],\n",
              " [9, 8, 0.20722513832820808, tensor(91.9355, device='cuda:0')],\n",
              " [9, 9, 0.1858182937890712, tensor(95.5645, device='cuda:0')]]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Loss_Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFuYhpscYtGp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}